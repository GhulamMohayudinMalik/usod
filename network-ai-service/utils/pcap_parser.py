"""
PCAP File Parser
Parses PCAP files and extracts network flows for analysis
"""

import os
import logging
from typing import List, Dict, Any, Optional, Generator
import pandas as pd
from datetime import datetime
import subprocess
import tempfile

logger = logging.getLogger(__name__)

class PCAPParser:
    """
    Parser for PCAP files to extract network flows
    """
    
    def __init__(self, tshark_path: Optional[str] = None):
        """
        Initialize PCAP parser
        
        Args:
            tshark_path: Path to tshark executable (optional)
        """
        self.tshark_path = tshark_path or self._find_tshark()
        self.temp_dir = tempfile.mkdtemp()
        
        if not self.tshark_path:
            logger.warning("tshark not found. Some features may not work.")
    
    def _find_tshark(self) -> Optional[str]:
        """Find tshark executable"""
        try:
            result = subprocess.run(['tshark', '--version'], 
                                  capture_output=True, text=True, timeout=5)
            if result.returncode == 0:
                return 'tshark'
        except (subprocess.TimeoutExpired, FileNotFoundError):
            pass
        
        # Try common installation paths
        common_paths = [
            r'C:\Program Files\Wireshark\tshark.exe',
            r'C:\Program Files (x86)\Wireshark\tshark.exe',
            '/usr/bin/tshark',
            '/usr/local/bin/tshark'
        ]
        
        for path in common_paths:
            if os.path.exists(path):
                return path
        
        return None
    
    def parse_pcap_file(self, pcap_path: str, 
                       output_format: str = 'csv') -> str:
        """
        Parse PCAP file and extract flow information
        
        Args:
            pcap_path: Path to PCAP file
            output_format: Output format ('csv', 'json')
            
        Returns:
            Path to extracted data file
        """
        if not os.path.exists(pcap_path):
            raise FileNotFoundError(f"PCAP file not found: {pcap_path}")
        
        if not self.tshark_path:
            raise RuntimeError("tshark not available for PCAP parsing")
        
        logger.info(f"Parsing PCAP file: {pcap_path}")
        
        # Generate output filename
        base_name = os.path.splitext(os.path.basename(pcap_path))[0]
        output_file = os.path.join(self.temp_dir, f"{base_name}_flows.{output_format}")
        
        # Tshark command to extract flow information
        if output_format == 'csv':
            tshark_cmd = [
                self.tshark_path,
                '-r', pcap_path,
                '-T', 'fields',
                '-e', 'frame.time',
                '-e', 'ip.src',
                '-e', 'ip.dst',
                '-e', 'tcp.srcport',
                '-e', 'tcp.dstport',
                '-e', 'udp.srcport',
                '-e', 'udp.dstport',
                '-e', 'ip.proto',
                '-e', 'frame.len',
                '-e', 'tcp.flags',
                '-E', 'header=y',
                '-E', 'separator=,'
            ]
        else:  # json
            tshark_cmd = [
                self.tshark_path,
                '-r', pcap_path,
                '-T', 'json',
                '-e', 'frame.time',
                '-e', 'ip.src',
                '-e', 'ip.dst',
                '-e', 'tcp.srcport',
                '-e', 'tcp.dstport',
                '-e', 'udp.srcport',
                '-e', 'udp.dstport',
                '-e', 'ip.proto',
                '-e', 'frame.len',
                '-e', 'tcp.flags'
            ]
        
        try:
            with open(output_file, 'w') as f:
                result = subprocess.run(tshark_cmd, stdout=f, stderr=subprocess.PIPE, 
                                      text=True, timeout=300)  # 5 minute timeout
            
            if result.returncode != 0:
                logger.error(f"tshark error: {result.stderr}")
                raise RuntimeError(f"tshark failed: {result.stderr}")
            
            logger.info(f"PCAP parsing completed. Output: {output_file}")
            return output_file
            
        except subprocess.TimeoutExpired:
            logger.error("PCAP parsing timed out")
            raise RuntimeError("PCAP parsing timed out")
        except Exception as e:
            logger.error(f"Error parsing PCAP file: {str(e)}")
            raise
    
    def load_flows_from_csv(self, csv_path: str) -> pd.DataFrame:
        """
        Load flows from CSV file generated by tshark
        
        Args:
            csv_path: Path to CSV file
            
        Returns:
            DataFrame with flow data
        """
        logger.info(f"Loading flows from CSV: {csv_path}")
        
        # Read CSV file
        df = pd.read_csv(csv_path)
        
        # Clean and standardize column names
        df.columns = df.columns.str.strip()
        
        # Map tshark columns to our standard format
        column_mapping = {
            'frame.time': 'timestamp',
            'ip.src': 'src_ip',
            'ip.dst': 'dst_ip',
            'tcp.srcport': 'src_port',
            'tcp.dstport': 'dst_port',
            'udp.srcport': 'src_port',
            'udp.dstport': 'dst_port',
            'ip.proto': 'protocol',
            'frame.len': 'packet_size',
            'tcp.flags': 'tcp_flags'
        }
        
        # Rename columns
        df = df.rename(columns=column_mapping)
        
        # Clean data
        df = self._clean_flow_data(df)
        
        logger.info(f"Loaded {len(df)} packets from CSV")
        return df
    
    def _clean_flow_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """Clean and standardize flow data"""
        # Remove rows with missing essential data
        df = df.dropna(subset=['src_ip', 'dst_ip'])
        
        # Fill missing ports with 0
        df['src_port'] = df['src_port'].fillna(0).astype(int)
        df['dst_port'] = df['dst_port'].fillna(0).astype(int)
        
        # Fill missing protocol with 0
        df['protocol'] = df['protocol'].fillna(0).astype(int)
        
        # Fill missing packet size with 0
        df['packet_size'] = df['packet_size'].fillna(0).astype(int)
        
        # Fill missing TCP flags with 0
        df['tcp_flags'] = df['tcp_flags'].fillna(0).astype(int)
        
        # Convert timestamp to datetime
        if 'timestamp' in df.columns:
            df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')
        
        # Map protocol numbers to names
        protocol_map = {1: 'ICMP', 6: 'TCP', 17: 'UDP'}
        df['protocol_name'] = df['protocol'].map(protocol_map).fillna('Other')
        
        return df
    
    def extract_flows_from_packets(self, df: pd.DataFrame) -> List[Dict[str, Any]]:
        """
        Extract network flows from packet data
        
        Args:
            df: DataFrame with packet data
            
        Returns:
            List of flow dictionaries
        """
        logger.info("Extracting flows from packet data...")
        
        flows = {}
        
        for _, packet in df.iterrows():
            # Create flow key (5-tuple)
            src_ip = packet['src_ip']
            dst_ip = packet['dst_ip']
            src_port = packet['src_port']
            dst_port = packet['dst_port']
            protocol = packet['protocol_name']
            
            # Create bidirectional flow key
            if src_ip < dst_ip or (src_ip == dst_ip and src_port < dst_port):
                flow_key = f"{src_ip}:{src_port}-{dst_ip}:{dst_port}-{protocol}"
            else:
                flow_key = f"{dst_ip}:{dst_port}-{src_ip}:{src_port}-{protocol}"
            
            # Initialize flow if not exists
            if flow_key not in flows:
                flows[flow_key] = {
                    'flow_id': flow_key,
                    'src_ip': src_ip,
                    'dst_ip': dst_ip,
                    'src_port': src_port,
                    'dst_port': dst_port,
                    'protocol': protocol,
                    'packet_count': 0,
                    'byte_count': 0,
                    'start_time': packet['timestamp'],
                    'end_time': packet['timestamp'],
                    'tcp_flags': set(),
                    'packet_sizes': []
                }
            
            # Update flow statistics
            flow = flows[flow_key]
            flow['packet_count'] += 1
            flow['byte_count'] += packet['packet_size']
            flow['end_time'] = packet['timestamp']
            flow['packet_sizes'].append(packet['packet_size'])
            
            # Update TCP flags
            if 'tcp_flags' in packet and pd.notna(packet['tcp_flags']):
                flow['tcp_flags'].add(packet['tcp_flags'])
        
        # Convert flows to list and calculate derived features
        flow_list = []
        for flow in flows.values():
            # Calculate duration
            if pd.notna(flow['start_time']) and pd.notna(flow['end_time']):
                duration = (flow['end_time'] - flow['start_time']).total_seconds()
            else:
                duration = 0
            
            flow['duration'] = duration
            
            # Calculate rates
            if duration > 0:
                flow['packets_per_second'] = flow['packet_count'] / duration
                flow['bytes_per_second'] = flow['byte_count'] / duration
            else:
                flow['packets_per_second'] = 0
                flow['bytes_per_second'] = 0
            
            # Calculate average packet size
            if flow['packet_count'] > 0:
                flow['avg_packet_size'] = flow['byte_count'] / flow['packet_count']
            else:
                flow['avg_packet_size'] = 0
            
            # Convert TCP flags set to count
            flow['tcp_flag_count'] = len(flow['tcp_flags'])
            
            # Remove temporary fields
            del flow['packet_sizes']
            del flow['tcp_flags']
            
            flow_list.append(flow)
        
        logger.info(f"Extracted {len(flow_list)} flows")
        return flow_list
    
    def parse_pcap_to_flows(self, pcap_path: str) -> List[Dict[str, Any]]:
        """
        Complete pipeline: Parse PCAP file and extract flows
        
        Args:
            pcap_path: Path to PCAP file
            
        Returns:
            List of flow dictionaries
        """
        logger.info(f"Starting complete PCAP to flows pipeline: {pcap_path}")
        
        # Parse PCAP file
        csv_path = self.parse_pcap_file(pcap_path, output_format='csv')
        
        # Load packet data
        df = self.load_flows_from_csv(csv_path)
        
        # Extract flows
        flows = self.extract_flows_from_packets(df)
        
        # Clean up temporary file
        try:
            os.remove(csv_path)
        except OSError:
            pass
        
        logger.info(f"PCAP to flows pipeline completed. Generated {len(flows)} flows.")
        return flows
    
    def cleanup(self):
        """Clean up temporary files"""
        try:
            import shutil
            shutil.rmtree(self.temp_dir)
            logger.info("Temporary files cleaned up")
        except OSError:
            pass

# Example usage
if __name__ == "__main__":
    # Configure logging
    logging.basicConfig(level=logging.INFO)
    
    # Create parser
    parser = PCAPParser()
    
    # Example usage (requires a PCAP file)
    pcap_file = "sample.pcap"  # Replace with actual PCAP file
    
    if os.path.exists(pcap_file):
        try:
            flows = parser.parse_pcap_to_flows(pcap_file)
            print(f"Extracted {len(flows)} flows")
            
            # Print first few flows
            for i, flow in enumerate(flows[:3]):
                print(f"Flow {i+1}: {flow['src_ip']}:{flow['src_port']} -> {flow['dst_ip']}:{flow['dst_port']} ({flow['protocol']})")
                print(f"  Packets: {flow['packet_count']}, Bytes: {flow['byte_count']}, Duration: {flow['duration']:.2f}s")
        except Exception as e:
            print(f"Error parsing PCAP file: {e}")
    else:
        print(f"PCAP file not found: {pcap_file}")
        print("This is a demonstration. In practice, you would provide a real PCAP file.")
    
    # Cleanup
    parser.cleanup()
