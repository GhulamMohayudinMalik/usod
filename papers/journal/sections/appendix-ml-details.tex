This appendix provides detailed technical specifications of the machine learning models, training procedures, hyperparameters, and performance characteristics of USOD's AI-enhanced network threat detection system.

\subsection{Dataset: CICIDS2017}

The Canadian Institute for Cybersecurity Intrusion Detection System 2017 (CICIDS2017) dataset provides labeled network traffic flows for training and evaluation.

\subsubsection{Dataset Characteristics}

\begin{table}[h]
\centering
\caption{CICIDS2017 Dataset Overview}
\label{tab:cicids-overview}
\begin{tabular}{|l|r|}
\hline
\textbf{Characteristic} & \textbf{Value} \\
\hline
Total flows & 2,830,743 \\
Collection period & July 3-7, 2017 (5 days) \\
Original features & 78 network flow features \\
Attack types & 14 categories \\
Benign traffic percentage & ~80\% \\
Attack traffic percentage & ~20\% \\
File size (compressed) & ~6.7 GB \\
File format & CSV \\
\hline
\end{tabular}
\end{table}

\subsubsection{Attack Type Distribution}

The dataset includes diverse attack types with varying representation:

\begin{itemize}
    \item \textbf{BENIGN}: 2,273,097 flows (80.3\%)
    \item \textbf{DoS/DDoS}: 252,672 flows (8.9\%)
    \item \textbf{PortScan}: 158,930 flows (5.6\%)
    \item \textbf{Bot}: 1,966 flows (0.07\%)
    \item \textbf{Infiltration}: 36 flows (0.001\%)
    \item \textbf{Web Attack}: 2,180 flows (0.08\%)
    \item \textbf{Brute Force}: 13,835 flows (0.49\%)
    \item \textbf{Heartbleed}: 11 flows (0.0004\%)
\end{itemize}

Note the severe class imbalance, with benign traffic dominating and some attack types (Heartbleed, Infiltration) extremely underrepresented.

\subsection{Feature Engineering}

USOD extracts 25 key features from the 78 original CICIDS2017 features for optimized performance:

\subsubsection{Selected Features}

\textbf{Flow-Level Features} (7):
\begin{itemize}
    \item Flow Duration: Total time duration of the flow
    \item Total Fwd Packets: Total packets in forward direction
    \item Total Bwd Packets: Total packets in backward direction
    \item Total Length of Fwd Packets: Total bytes forwarded
    \item Total Length of Bwd Packets: Total bytes backward
    \item Flow Bytes/s: Flow byte rate
    \item Flow Packets/s: Flow packet rate
\end{itemize}

\textbf{Packet-Level Statistics} (8):
\begin{itemize}
    \item Fwd Packet Length Mean: Average forward packet size
    \item Fwd Packet Length Max: Maximum forward packet size
    \item Fwd Packet Length Min: Minimum forward packet size
    \item Fwd Packet Length Std: Standard deviation of forward packet sizes
    \item Bwd Packet Length Mean: Average backward packet size
    \item Bwd Packet Length Max: Maximum backward packet size
    \item Bwd Packet Length Min: Minimum backward packet size
    \item Bwd Packet Length Std: Standard deviation of backward packet sizes
\end{itemize}

\textbf{Inter-Arrival Time} (4):
\begin{itemize}
    \item Flow IAT Mean: Mean inter-arrival time between packets
    \item Flow IAT Std: Standard deviation of inter-arrival times
    \item Flow IAT Max: Maximum inter-arrival time
    \item Flow IAT Min: Minimum inter-arrival time
\end{itemize}

\textbf{TCP Flags} (4):
\begin{itemize}
    \item FIN Flag Count: Number of FIN flags
    \item SYN Flag Count: Number of SYN flags
    \item RST Flag Count: Number of RST flags
    \item PSH Flag Count: Number of PSH flags
\end{itemize}

\textbf{Advanced Features} (2):
\begin{itemize}
    \item Down/Up Ratio: Ratio of downstream to upstream bytes
    \item Average Packet Size: Mean packet size across flow
\end{itemize}

\subsubsection{Feature Selection Methodology}

Features were selected through:
\begin{enumerate}
    \item \textbf{Correlation Analysis}: Removing highly correlated features (>0.95 correlation)
    \item \textbf{Feature Importance}: Random Forest feature importance ranking
    \item \textbf{Domain Knowledge}: Including features known to be discriminative for network attacks
    \item \textbf{Performance Testing}: Empirical validation showing 25 features achieve 99.9\% of 78-feature accuracy with 3x faster inference
\end{enumerate}

\subsection{Data Preprocessing Pipeline}

\subsubsection{Missing Value Handling}

\begin{lstlisting}[language=Python, caption=Missing Value Treatment, basicstyle=\footnotesize\ttfamily, breaklines=true]
# Replace infinite values with NaN
df.replace([np.inf, -np.inf], np.nan, inplace=True)

# Fill NaN with 0 (conservative for network flows)
df.fillna(0, inplace=True)
\end{lstlisting}

\subsubsection{Feature Scaling}

StandardScaler normalization for all numerical features:

\begin{lstlisting}[language=Python, caption=Feature Scaling, basicstyle=\footnotesize\ttfamily, breaklines=true]
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_train)

# Formula: X' = (X - mean) / std_dev
\end{lstlisting}

Scaling parameters are saved with the model for consistent inference-time scaling.

\subsubsection{Label Encoding}

Attack type labels are encoded numerically:

\begin{lstlisting}[language=Python, caption=Label Encoding, basicstyle=\footnotesize\ttfamily, breaklines=true]
from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y_train)

# Example mapping:
# BENIGN -> 0
# Bot -> 1
# DDoS -> 2
# DoS slowloris -> 3
# FTP-Patator -> 4
# PortScan -> 5
\end{lstlisting}

\subsubsection{Class Imbalance Handling}

Two strategies address class imbalance:

\textbf{Class Weighting}:
\begin{lstlisting}[language=Python, caption=Class Weight Calculation, basicstyle=\footnotesize\ttfamily, breaklines=true]
from sklearn.utils.class_weight import compute_class_weight

class_weights = compute_class_weight(
    'balanced',
    classes=np.unique(y_train),
    y=y_train
)

# Results in higher weight for minority classes
# Example: BENIGN=0.5, Heartbleed=100.0
\end{lstlisting}

\textbf{SMOTE (Synthetic Minority Over-sampling)} (optional):
\begin{lstlisting}[language=Python, caption=SMOTE Implementation, basicstyle=\footnotesize\ttfamily, breaklines=true]
from imblearn.over_sampling import SMOTE

smote = SMOTE(sampling_strategy='minority', random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)
\end{lstlisting}

\subsection{Random Forest Model}

\subsubsection{Hyperparameters}

\begin{table}[h]
\centering
\caption{Random Forest Hyperparameters}
\label{tab:rf-hyperparams}
\begin{tabular}{|l|l|p{6cm}|}
\hline
\textbf{Parameter} & \textbf{Value} & \textbf{Rationale} \\
\hline
n\_estimators & 100-200 & Balance between accuracy and training time \\
max\_depth & None & Allow trees to grow until pure leaves for complex patterns \\
min\_samples\_split & 2 & Default splitting threshold \\
min\_samples\_leaf & 1 & Allow fine-grained decision boundaries \\
max\_features & 'sqrt' & Use sqrt(25) â‰ˆ 5 features per split \\
class\_weight & 'balanced' & Handle class imbalance \\
random\_state & 42 & Reproducibility \\
n\_jobs & -1 & Use all CPU cores for training \\
bootstrap & True & Enable bagging for variance reduction \\
criterion & 'gini' & Gini impurity for split quality \\
\hline
\end{tabular}
\end{table}

\subsubsection{Training Procedure}

\begin{lstlisting}[language=Python, caption=Random Forest Training, basicstyle=\footnotesize\ttfamily, breaklines=true]
from sklearn.ensemble import RandomForestClassifier

rf_model = RandomForestClassifier(
    n_estimators=100,
    max_depth=None,
    min_samples_split=2,
    min_samples_leaf=1,
    max_features='sqrt',
    class_weight='balanced',
    random_state=42,
    n_jobs=-1,
    verbose=1
)

# Training
rf_model.fit(X_train_scaled, y_train)

# Training time: ~180-240 seconds on CICIDS2017
\end{lstlisting}

\subsubsection{Performance Metrics}

\begin{table}[h]
\centering
\caption{Random Forest Performance on CICIDS2017 Test Set}
\label{tab:rf-performance}
\begin{tabular}{|l|r|r|r|r|}
\hline
\textbf{Class} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} & \textbf{Support} \\
\hline
BENIGN & 1.00 & 1.00 & 1.00 & 454,619 \\
Bot & 1.00 & 0.98 & 0.99 & 393 \\
DDoS & 1.00 & 1.00 & 1.00 & 25,267 \\
DoS slowloris & 1.00 & 0.99 & 0.99 & 1,093 \\
FTP-Patator & 1.00 & 0.99 & 1.00 & 1,542 \\
PortScan & 1.00 & 1.00 & 1.00 & 31,786 \\
\hline
\textbf{Accuracy} & \multicolumn{4}{l|}{99.97\%} \\
\textbf{Macro Avg} & 1.00 & 0.99 & 1.00 & 514,700 \\
\textbf{Weighted Avg} & 1.00 & 1.00 & 1.00 & 514,700 \\
\hline
\end{tabular}
\end{table}

\subsubsection{Feature Importance}

Top 10 most important features:

\begin{enumerate}
    \item Fwd Packet Length Mean (0.18)
    \item Flow Duration (0.15)
    \item Flow IAT Mean (0.12)
    \item Total Fwd Packets (0.11)
    \item Bwd Packet Length Mean (0.09)
    \item Flow Bytes/s (0.08)
    \item Down/Up Ratio (0.07)
    \item Total Length of Fwd Packets (0.06)
    \item SYN Flag Count (0.05)
    \item Flow IAT Std (0.04)
\end{enumerate}

\subsection{Isolation Forest Model}

\subsubsection{Hyperparameters}

\begin{table}[h]
\centering
\caption{Isolation Forest Hyperparameters}
\label{tab:if-hyperparams}
\begin{tabular}{|l|l|p{6cm}|}
\hline
\textbf{Parameter} & \textbf{Value} & \textbf{Rationale} \\
\hline
n\_estimators & 100-200 & Number of isolation trees \\
max\_samples & 256 & Subsample size for each tree \\
contamination & 0.1-0.2 & Expected proportion of anomalies (10-20\%) \\
max\_features & 1.0 & Use all features \\
bootstrap & False & Sample without replacement \\
random\_state & 42 & Reproducibility \\
n\_jobs & -1 & Parallel tree building \\
\hline
\end{tabular}
\end{table}

\subsubsection{Anomaly Score Calculation}

Isolation Forest assigns anomaly scores based on path length:

\begin{equation}
s(x, n) = 2^{-\frac{E(h(x))}{c(n)}}
\end{equation}

Where:
\begin{itemize}
    \item $h(x)$ is the path length for instance $x$
    \item $E(h(x))$ is the average path length across all trees
    \item $c(n)$ is the average path length of unsuccessful search in BST
    \item $n$ is the number of external nodes
\end{itemize}

Interpretation:
\begin{itemize}
    \item Score near 1: Definite anomaly
    \item Score $<$ 0.5: Normal instance
    \item Score around 0.5: Ambiguous (needs further investigation)
\end{itemize}

\subsubsection{Performance Metrics}

\begin{table}[h]
\centering
\caption{Isolation Forest Performance}
\label{tab:if-performance}
\begin{tabular}{|l|r|}
\hline
\textbf{Metric} & \textbf{Value} \\
\hline
Overall Accuracy & 87.33\% \\
True Positive Rate (Anomaly Detection) & 72.5\% \\
False Positive Rate & 8.2\% \\
Training Time & 45-60 seconds \\
Inference Time per Flow & 28.3ms \\
\hline
\end{tabular}
\end{table}

\subsection{Model Persistence and Deployment}

\subsubsection{Model Serialization}

Models are saved using joblib for efficient serialization:

\begin{lstlisting}[language=Python, caption=Model Saving, basicstyle=\footnotesize\ttfamily, breaklines=true]
import joblib

# Save models
joblib.dump(rf_model, 'random_forest_model.pkl')
joblib.dump(if_model, 'isolation_forest_model.pkl')

# Save preprocessing components
joblib.dump(scaler, 'scaler.pkl')
joblib.dump(label_encoder, 'label_encoder.pkl')

# Model file sizes:
# random_forest_model.pkl: ~45 MB
# isolation_forest_model.pkl: ~38 MB
# scaler.pkl: ~2 KB
# label_encoder.pkl: ~1 KB
\end{lstlisting}

\subsubsection{Model Loading and Inference}

\begin{lstlisting}[language=Python, caption=Model Inference, basicstyle=\footnotesize\ttfamily, breaklines=true]
# Load models
rf_model = joblib.load('random_forest_model.pkl')
scaler = joblib.load('scaler.pkl')
label_encoder = joblib.load('label_encoder.pkl')

# Inference
def predict_threat(flow_features):
    # Scale features
    features_scaled = scaler.transform([flow_features])
    
    # Predict
    prediction = rf_model.predict(features_scaled)[0]
    probabilities = rf_model.predict_proba(features_scaled)[0]
    
    # Decode label
    threat_type = label_encoder.inverse_transform([prediction])[0]
    confidence = probabilities[prediction]
    
    return {
        'threat_type': threat_type,
        'confidence': float(confidence),
        'all_probabilities': dict(zip(
            label_encoder.classes_, 
            probabilities
        ))
    }

# Average inference time: 34.51ms per flow
\end{lstlisting}

\subsection{Training Performance}

\subsubsection{Training Time Breakdown}

Using model\_training\_fast.py on standard development workstation:

\begin{table}[h]
\centering
\caption{Training Pipeline Performance}
\label{tab:training-performance}
\begin{tabular}{|l|r|}
\hline
\textbf{Stage} & \textbf{Time} \\
\hline
Data loading (CSV parsing) & 45-60 seconds \\
Preprocessing and scaling & 15-20 seconds \\
Feature selection & 5-8 seconds \\
Random Forest training & 180-240 seconds \\
Isolation Forest training & 45-60 seconds \\
Model evaluation & 20-30 seconds \\
Model saving & 5-10 seconds \\
\hline
\textbf{Total} & \textbf{315-428 seconds (5-7 min)} \\
\hline
\end{tabular}
\end{table}

\subsubsection{Hardware Specifications}

Training conducted on:
\begin{itemize}
    \item CPU: Intel Core i7 (8 cores) or equivalent
    \item RAM: 16 GB minimum (32 GB recommended)
    \item Storage: SSD recommended for faster data loading
    \item GPU: Not utilized (scikit-learn CPU-only)
\end{itemize}

\subsection{Model Limitations and Considerations}

\subsubsection{Dataset Age}

\textbf{Critical Limitation}: Models trained exclusively on CICIDS2017 (July 2017) show degraded performance on contemporary threats:

\begin{itemize}
    \item Modern malware samples (2023-2024): 2-19\% confidence
    \item Contemporary DDoS patterns: 15-35\% confidence
    \item Zero-day exploits: Unpredictable performance
\end{itemize}

\textbf{Recommendation}: Retrain models quarterly with recent attack captures from production environments, honeypots, or updated datasets (CICIDS2019, CSE-CIC-IDS2018).

\subsubsection{Class Imbalance Impact}

Despite class weighting, minority classes (Heartbleed: 11 samples, Infiltration: 36 samples) have limited representation. Models may struggle with these attack types in production.

\subsubsection{Feature Drift}

Network infrastructure changes (faster networks, different protocols, encrypted traffic) can cause feature distribution drift, degrading model accuracy over time.

\textbf{Mitigation}: Implement model monitoring tracking prediction confidence over time and retraining when confidence drops below threshold (e.g., 80\%).

\subsection{Model Monitoring and Maintenance}

Production deployment should implement:

\begin{itemize}
    \item \textbf{Prediction Confidence Tracking}: Log confidence scores for drift detection
    \item \textbf{Confusion Matrix Updates}: Periodically evaluate on labeled production data
    \item \textbf{Model Versioning}: MLflow or DVC for model lifecycle management
    \item \textbf{A/B Testing}: Gradual rollout of updated models with performance comparison
    \item \textbf{Retraining Pipeline}: Automated retraining with new data samples
    \item \textbf{Model Explainability}: SHAP or LIME for prediction explanations
\end{itemize}

These ML implementation details demonstrate the technical depth of USOD's AI-enhanced threat detection while acknowledging practical limitations requiring ongoing model maintenance for production effectiveness.

