{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# USOD Technical Paper - Comprehensive ML Model Training\n",
                "\n",
                "This notebook trains multiple machine learning models on the CICIDS2017 dataset for the USOD technical paper.\n",
                "\n",
                "**Models Trained:**\n",
                "1. Random Forest\n",
                "2. Extra Trees\n",
                "3. XGBoost\n",
                "4. LightGBM\n",
                "5. CatBoost\n",
                "6. Isolation Forest (Anomaly Detection)\n",
                "\n",
                "**Output:**\n",
                "- Confusion matrices for each model\n",
                "- ROC curves comparison\n",
                "- Feature importance plots\n",
                "- Performance metrics table"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 1. Setup and Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required packages if needed\n",
                "# !pip install xgboost lightgbm catboost scikit-learn pandas matplotlib seaborn"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from pathlib import Path\n",
                "import warnings\n",
                "import time\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Sklearn imports\n",
                "from sklearn.model_selection import train_test_split, cross_val_score\n",
                "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, IsolationForest\n",
                "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
                "from sklearn.metrics import (\n",
                "    classification_report, confusion_matrix, accuracy_score,\n",
                "    precision_score, recall_score, f1_score, roc_curve, auc\n",
                ")\n",
                "import joblib\n",
                "\n",
                "# Boosting libraries\n",
                "try:\n",
                "    from xgboost import XGBClassifier\n",
                "    print(\"‚úÖ XGBoost imported\")\n",
                "except ImportError:\n",
                "    print(\"‚ùå XGBoost not installed. Run: pip install xgboost\")\n",
                "    XGBClassifier = None\n",
                "\n",
                "try:\n",
                "    from lightgbm import LGBMClassifier\n",
                "    print(\"‚úÖ LightGBM imported\")\n",
                "except ImportError:\n",
                "    print(\"‚ùå LightGBM not installed. Run: pip install lightgbm\")\n",
                "    LGBMClassifier = None\n",
                "\n",
                "try:\n",
                "    from catboost import CatBoostClassifier\n",
                "    print(\"‚úÖ CatBoost imported\")\n",
                "except ImportError:\n",
                "    print(\"‚ùå CatBoost not installed. Run: pip install catboost\")\n",
                "    CatBoostClassifier = None\n",
                "\n",
                "# Set style for paper-quality figures\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "plt.rcParams['figure.figsize'] = (10, 8)\n",
                "plt.rcParams['font.size'] = 12\n",
                "plt.rcParams['axes.labelsize'] = 14\n",
                "plt.rcParams['axes.titlesize'] = 16\n",
                "\n",
                "print(\"\\n‚úÖ All core libraries imported successfully\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create output directory\n",
                "OUTPUT_DIR = Path(\"paper_results\")\n",
                "OUTPUT_DIR.mkdir(exist_ok=True)\n",
                "\n",
                "MODEL_DIR = OUTPUT_DIR / \"models\"\n",
                "MODEL_DIR.mkdir(exist_ok=True)\n",
                "\n",
                "print(f\"üìÅ Results will be saved to: {OUTPUT_DIR.absolute()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 2. Load CICIDS2017 Dataset (All CSVs Once)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Dataset path\n",
                "DATA_DIR = Path(\"data/raw\")\n",
                "\n",
                "# List all CSV files\n",
                "csv_files = list(DATA_DIR.glob(\"*.csv\"))\n",
                "print(f\"Found {len(csv_files)} CSV files:\")\n",
                "total_size = 0\n",
                "for f in csv_files:\n",
                "    size_mb = f.stat().st_size / (1024 * 1024)\n",
                "    total_size += size_mb\n",
                "    print(f\"  üìÑ {f.name}: {size_mb:.1f} MB\")\n",
                "print(f\"\\nüìä Total: {total_size:.1f} MB\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load all files and combine - THIS RUNS ONCE\n",
                "print(\"Loading all CSV files... (this may take a few minutes)\")\n",
                "start_time = time.time()\n",
                "\n",
                "dfs = []\n",
                "for csv_file in csv_files:\n",
                "    print(f\"  Loading {csv_file.name}...\", end=\" \")\n",
                "    try:\n",
                "        df = pd.read_csv(csv_file, encoding='utf-8', low_memory=False)\n",
                "        df.columns = df.columns.str.strip()  # Clean column names\n",
                "        dfs.append(df)\n",
                "        print(f\"‚úÖ {len(df):,} rows\")\n",
                "    except Exception as e:\n",
                "        print(f\"‚ùå Error: {e}\")\n",
                "\n",
                "# Combine all dataframes\n",
                "data = pd.concat(dfs, ignore_index=True)\n",
                "load_time = time.time() - start_time\n",
                "\n",
                "print(f\"\\n‚úÖ Dataset loaded in {load_time:.1f} seconds\")\n",
                "print(f\"üìä Total: {len(data):,} rows, {len(data.columns)} columns\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Preview data\n",
                "data.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 3. Data Exploration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Label distribution\n",
                "print(\"üìä Attack Type Distribution:\")\n",
                "label_counts = data['Label'].value_counts()\n",
                "print(label_counts)\n",
                "print(f\"\\nTotal classes: {len(label_counts)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize attack distribution\n",
                "plt.figure(figsize=(14, 8))\n",
                "colors = ['#27ae60' if 'BENIGN' in label else '#e74c3c' for label in label_counts.index]\n",
                "bars = plt.barh(label_counts.index, label_counts.values, color=colors)\n",
                "\n",
                "# Add value labels\n",
                "for bar, value in zip(bars, label_counts.values):\n",
                "    plt.text(value + 1000, bar.get_y() + bar.get_height()/2, \n",
                "             f'{value:,}', va='center', fontsize=10)\n",
                "\n",
                "plt.xlabel('Number of Samples (log scale)')\n",
                "plt.xscale('log')\n",
                "plt.title('CICIDS2017 Dataset - Attack Type Distribution')\n",
                "plt.tight_layout()\n",
                "plt.savefig(OUTPUT_DIR / 'dataset_attack_distribution.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "print(f\"‚úÖ Saved: dataset_attack_distribution.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 4. Data Preprocessing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clean data\n",
                "print(f\"Before cleaning: {len(data):,} rows\")\n",
                "\n",
                "# Replace infinity values\n",
                "data = data.replace([np.inf, -np.inf], np.nan)\n",
                "\n",
                "# Drop rows with NaN\n",
                "data = data.dropna()\n",
                "\n",
                "print(f\"After cleaning: {len(data):,} rows\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Separate features and labels\n",
                "X = data.drop(columns=['Label'])\n",
                "y = data['Label']\n",
                "\n",
                "# Keep only numeric columns\n",
                "numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
                "X = X[numeric_cols]\n",
                "\n",
                "print(f\"Features: {len(numeric_cols)} numeric columns\")\n",
                "print(f\"Samples: {len(X):,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Encode labels for multiclass\n",
                "label_encoder = LabelEncoder()\n",
                "y_encoded = label_encoder.fit_transform(y)\n",
                "class_names = label_encoder.classes_\n",
                "\n",
                "print(f\"Classes ({len(class_names)}): {list(class_names)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create binary labels (BENIGN=0, ATTACK=1)\n",
                "y_binary = np.where(y == 'BENIGN', 0, 1)\n",
                "\n",
                "print(f\"Binary distribution:\")\n",
                "print(f\"  BENIGN (0): {np.sum(y_binary == 0):,}\")\n",
                "print(f\"  ATTACK (1): {np.sum(y_binary == 1):,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Sample for faster training - ADJUST THIS VALUE\n",
                "SAMPLE_SIZE = 300000  # Increase for better results, decrease for faster training\n",
                "\n",
                "if len(X) > SAMPLE_SIZE:\n",
                "    print(f\"‚ö° Sampling {SAMPLE_SIZE:,} rows for training (stratified)...\")\n",
                "    X_sampled, _, y_multi_sampled, _, y_bin_sampled, _ = train_test_split(\n",
                "        X, y_encoded, y_binary,\n",
                "        train_size=SAMPLE_SIZE,\n",
                "        stratify=y_encoded,\n",
                "        random_state=42\n",
                "    )\n",
                "    X_sampled = X_sampled.values\n",
                "else:\n",
                "    X_sampled = X.values\n",
                "    y_multi_sampled = y_encoded\n",
                "    y_bin_sampled = y_binary\n",
                "\n",
                "print(f\"Training set size: {len(X_sampled):,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Scale features\n",
                "print(\"Scaling features...\")\n",
                "scaler = StandardScaler()\n",
                "X_scaled = scaler.fit_transform(X_sampled)\n",
                "print(\"‚úÖ Features scaled\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train/Test split - MULTICLASS\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X_scaled, y_multi_sampled,\n",
                "    test_size=0.2,\n",
                "    random_state=42,\n",
                "    stratify=y_multi_sampled\n",
                ")\n",
                "\n",
                "# Train/Test split - BINARY\n",
                "X_train_bin, X_test_bin, y_train_bin, y_test_bin = train_test_split(\n",
                "    X_scaled, y_bin_sampled,\n",
                "    test_size=0.2,\n",
                "    random_state=42,\n",
                "    stratify=y_bin_sampled\n",
                ")\n",
                "\n",
                "print(f\"Train set: {len(X_train):,} samples\")\n",
                "print(f\"Test set: {len(X_test):,} samples\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 5. Helper Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Dictionary to store all results\n",
                "results = {}\n",
                "\n",
                "def evaluate_model(model, X_test, y_test, model_name, is_binary=False):\n",
                "    \"\"\"Evaluate model and store results\"\"\"\n",
                "    y_pred = model.predict(X_test)\n",
                "    \n",
                "    if is_binary:\n",
                "        y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
                "    else:\n",
                "        y_proba = model.predict_proba(X_test) if hasattr(model, 'predict_proba') else None\n",
                "    \n",
                "    metrics = {\n",
                "        'accuracy': accuracy_score(y_test, y_pred),\n",
                "        'precision': precision_score(y_test, y_pred, average='weighted', zero_division=0),\n",
                "        'recall': recall_score(y_test, y_pred, average='weighted', zero_division=0),\n",
                "        'f1': f1_score(y_test, y_pred, average='weighted', zero_division=0),\n",
                "        'predictions': y_pred,\n",
                "        'probabilities': y_proba\n",
                "    }\n",
                "    \n",
                "    results[model_name] = metrics\n",
                "    \n",
                "    print(f\"\\n{'='*50}\")\n",
                "    print(f\"{model_name} RESULTS\")\n",
                "    print(f\"{'='*50}\")\n",
                "    print(f\"Accuracy:  {metrics['accuracy']*100:.2f}%\")\n",
                "    print(f\"Precision: {metrics['precision']*100:.2f}%\")\n",
                "    print(f\"Recall:    {metrics['recall']*100:.2f}%\")\n",
                "    print(f\"F1-Score:  {metrics['f1']*100:.2f}%\")\n",
                "    \n",
                "    return metrics\n",
                "\n",
                "def plot_confusion_matrix(y_true, y_pred, class_labels, model_name, filename):\n",
                "    \"\"\"Plot and save confusion matrix\"\"\"\n",
                "    cm = confusion_matrix(y_true, y_pred)\n",
                "    \n",
                "    plt.figure(figsize=(14, 12))\n",
                "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
                "                xticklabels=class_labels, yticklabels=class_labels)\n",
                "    plt.xlabel('Predicted')\n",
                "    plt.ylabel('Actual')\n",
                "    acc = accuracy_score(y_true, y_pred)\n",
                "    plt.title(f'{model_name} Confusion Matrix (Accuracy: {acc*100:.2f}%)')\n",
                "    plt.tight_layout()\n",
                "    plt.savefig(OUTPUT_DIR / filename, dpi=300, bbox_inches='tight')\n",
                "    plt.show()\n",
                "    print(f\"‚úÖ Saved: {filename}\")\n",
                "\n",
                "def plot_feature_importance(model, feature_names, model_name, filename, top_n=15):\n",
                "    \"\"\"Plot feature importance\"\"\"\n",
                "    if hasattr(model, 'feature_importances_'):\n",
                "        importance = pd.DataFrame({\n",
                "            'feature': feature_names,\n",
                "            'importance': model.feature_importances_\n",
                "        }).sort_values('importance', ascending=False).head(top_n)\n",
                "        \n",
                "        plt.figure(figsize=(10, 8))\n",
                "        colors = plt.cm.RdYlGn(np.linspace(0.2, 0.8, len(importance)))[::-1]\n",
                "        plt.barh(importance['feature'], importance['importance'], color=colors)\n",
                "        plt.xlabel('Importance')\n",
                "        plt.title(f'{model_name} - Top {top_n} Feature Importance')\n",
                "        plt.gca().invert_yaxis()\n",
                "        plt.tight_layout()\n",
                "        plt.savefig(OUTPUT_DIR / filename, dpi=300, bbox_inches='tight')\n",
                "        plt.show()\n",
                "        print(f\"‚úÖ Saved: {filename}\")\n",
                "        return importance\n",
                "    else:\n",
                "        print(f\"‚ö†Ô∏è {model_name} does not have feature_importances_\")\n",
                "        return None"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 6. Model 1: Random Forest"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"üå≤ Training Random Forest...\")\n",
                "start = time.time()\n",
                "\n",
                "rf_model = RandomForestClassifier(\n",
                "    n_estimators=100,\n",
                "    max_depth=20,\n",
                "    min_samples_split=5,\n",
                "    min_samples_leaf=2,\n",
                "    n_jobs=-1,\n",
                "    random_state=42\n",
                ")\n",
                "rf_model.fit(X_train, y_train)\n",
                "\n",
                "train_time = time.time() - start\n",
                "print(f\"‚úÖ Random Forest trained in {train_time:.1f}s\")\n",
                "\n",
                "# Evaluate\n",
                "rf_metrics = evaluate_model(rf_model, X_test, y_test, 'Random Forest')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Random Forest Confusion Matrix\n",
                "plot_confusion_matrix(y_test, rf_metrics['predictions'], class_names, \n",
                "                      'Random Forest', 'rf_confusion_matrix.png')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Random Forest Feature Importance\n",
                "rf_importance = plot_feature_importance(rf_model, numeric_cols, \n",
                "                                        'Random Forest', 'rf_feature_importance.png')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 7. Model 2: Extra Trees"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"üå≥ Training Extra Trees...\")\n",
                "start = time.time()\n",
                "\n",
                "et_model = ExtraTreesClassifier(\n",
                "    n_estimators=100,\n",
                "    max_depth=20,\n",
                "    min_samples_split=5,\n",
                "    min_samples_leaf=2,\n",
                "    n_jobs=-1,\n",
                "    random_state=42\n",
                ")\n",
                "et_model.fit(X_train, y_train)\n",
                "\n",
                "train_time = time.time() - start\n",
                "print(f\"‚úÖ Extra Trees trained in {train_time:.1f}s\")\n",
                "\n",
                "# Evaluate\n",
                "et_metrics = evaluate_model(et_model, X_test, y_test, 'Extra Trees')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extra Trees Confusion Matrix\n",
                "plot_confusion_matrix(y_test, et_metrics['predictions'], class_names,\n",
                "                      'Extra Trees', 'et_confusion_matrix.png')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extra Trees Feature Importance\n",
                "et_importance = plot_feature_importance(et_model, numeric_cols,\n",
                "                                        'Extra Trees', 'et_feature_importance.png')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 8. Model 3: XGBoost"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if XGBClassifier is not None:\n",
                "    print(\"üöÄ Training XGBoost...\")\n",
                "    start = time.time()\n",
                "    \n",
                "    xgb_model = XGBClassifier(\n",
                "        n_estimators=100,\n",
                "        max_depth=10,\n",
                "        learning_rate=0.1,\n",
                "        subsample=0.8,\n",
                "        colsample_bytree=0.8,\n",
                "        n_jobs=-1,\n",
                "        random_state=42,\n",
                "        use_label_encoder=False,\n",
                "        eval_metric='mlogloss'\n",
                "    )\n",
                "    xgb_model.fit(X_train, y_train)\n",
                "    \n",
                "    train_time = time.time() - start\n",
                "    print(f\"‚úÖ XGBoost trained in {train_time:.1f}s\")\n",
                "    \n",
                "    # Evaluate\n",
                "    xgb_metrics = evaluate_model(xgb_model, X_test, y_test, 'XGBoost')\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è XGBoost not available. Install with: pip install xgboost\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if XGBClassifier is not None:\n",
                "    # XGBoost Confusion Matrix\n",
                "    plot_confusion_matrix(y_test, xgb_metrics['predictions'], class_names,\n",
                "                          'XGBoost', 'xgb_confusion_matrix.png')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if XGBClassifier is not None:\n",
                "    # XGBoost Feature Importance\n",
                "    xgb_importance = plot_feature_importance(xgb_model, numeric_cols,\n",
                "                                             'XGBoost', 'xgb_feature_importance.png')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 9. Model 4: LightGBM"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if LGBMClassifier is not None:\n",
                "    print(\"üí° Training LightGBM...\")\n",
                "    start = time.time()\n",
                "    \n",
                "    lgbm_model = LGBMClassifier(\n",
                "        n_estimators=100,\n",
                "        max_depth=10,\n",
                "        learning_rate=0.1,\n",
                "        subsample=0.8,\n",
                "        colsample_bytree=0.8,\n",
                "        n_jobs=-1,\n",
                "        random_state=42,\n",
                "        verbose=-1\n",
                "    )\n",
                "    lgbm_model.fit(X_train, y_train)\n",
                "    \n",
                "    train_time = time.time() - start\n",
                "    print(f\"‚úÖ LightGBM trained in {train_time:.1f}s\")\n",
                "    \n",
                "    # Evaluate\n",
                "    lgbm_metrics = evaluate_model(lgbm_model, X_test, y_test, 'LightGBM')\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è LightGBM not available. Install with: pip install lightgbm\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if LGBMClassifier is not None:\n",
                "    # LightGBM Confusion Matrix\n",
                "    plot_confusion_matrix(y_test, lgbm_metrics['predictions'], class_names,\n",
                "                          'LightGBM', 'lgbm_confusion_matrix.png')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if LGBMClassifier is not None:\n",
                "    # LightGBM Feature Importance\n",
                "    lgbm_importance = plot_feature_importance(lgbm_model, numeric_cols,\n",
                "                                              'LightGBM', 'lgbm_feature_importance.png')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 10. Model 5: CatBoost"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if CatBoostClassifier is not None:\n",
                "    print(\"üê± Training CatBoost...\")\n",
                "    start = time.time()\n",
                "    \n",
                "    catboost_model = CatBoostClassifier(\n",
                "        iterations=100,\n",
                "        depth=10,\n",
                "        learning_rate=0.1,\n",
                "        random_state=42,\n",
                "        verbose=False\n",
                "    )\n",
                "    catboost_model.fit(X_train, y_train)\n",
                "    \n",
                "    train_time = time.time() - start\n",
                "    print(f\"‚úÖ CatBoost trained in {train_time:.1f}s\")\n",
                "    \n",
                "    # Evaluate\n",
                "    catboost_metrics = evaluate_model(catboost_model, X_test, y_test, 'CatBoost')\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è CatBoost not available. Install with: pip install catboost\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if CatBoostClassifier is not None:\n",
                "    # CatBoost Confusion Matrix\n",
                "    plot_confusion_matrix(y_test, catboost_metrics['predictions'], class_names,\n",
                "                          'CatBoost', 'catboost_confusion_matrix.png')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if CatBoostClassifier is not None:\n",
                "    # CatBoost Feature Importance\n",
                "    catboost_importance = plot_feature_importance(catboost_model, numeric_cols,\n",
                "                                                  'CatBoost', 'catboost_feature_importance.png')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 11. Model 6: Isolation Forest (Anomaly Detection)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"üîç Training Isolation Forest (Anomaly Detection)...\")\n",
                "start = time.time()\n",
                "\n",
                "# Train only on BENIGN samples\n",
                "benign_mask = y_bin_sampled == 0\n",
                "X_benign = X_scaled[benign_mask]\n",
                "print(f\"Training on {len(X_benign):,} BENIGN samples\")\n",
                "\n",
                "iso_model = IsolationForest(\n",
                "    n_estimators=100,\n",
                "    contamination=0.1,\n",
                "    max_samples='auto',\n",
                "    random_state=42,\n",
                "    n_jobs=-1\n",
                ")\n",
                "iso_model.fit(X_benign)\n",
                "\n",
                "train_time = time.time() - start\n",
                "print(f\"‚úÖ Isolation Forest trained in {train_time:.1f}s\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Predict on test set (1=inlier/benign, -1=outlier/attack)\n",
                "iso_predictions = iso_model.predict(X_test_bin)\n",
                "iso_pred_binary = np.where(iso_predictions == 1, 0, 1)  # Convert to 0=benign, 1=attack\n",
                "\n",
                "# Get anomaly scores\n",
                "anomaly_scores = -iso_model.score_samples(X_test_bin)\n",
                "\n",
                "# Metrics\n",
                "iso_metrics = {\n",
                "    'accuracy': accuracy_score(y_test_bin, iso_pred_binary),\n",
                "    'precision': precision_score(y_test_bin, iso_pred_binary, zero_division=0),\n",
                "    'recall': recall_score(y_test_bin, iso_pred_binary, zero_division=0),\n",
                "    'f1': f1_score(y_test_bin, iso_pred_binary, zero_division=0),\n",
                "    'predictions': iso_pred_binary,\n",
                "    'probabilities': anomaly_scores\n",
                "}\n",
                "results['Isolation Forest'] = iso_metrics\n",
                "\n",
                "print(f\"\\n{'='*50}\")\n",
                "print(\"ISOLATION FOREST RESULTS\")\n",
                "print(f\"{'='*50}\")\n",
                "print(f\"Accuracy:  {iso_metrics['accuracy']*100:.2f}%\")\n",
                "print(f\"Precision: {iso_metrics['precision']*100:.2f}%\")\n",
                "print(f\"Recall:    {iso_metrics['recall']*100:.2f}%\")\n",
                "print(f\"F1-Score:  {iso_metrics['f1']*100:.2f}%\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Isolation Forest Confusion Matrix (Binary)\n",
                "cm_iso = confusion_matrix(y_test_bin, iso_pred_binary)\n",
                "\n",
                "plt.figure(figsize=(8, 6))\n",
                "sns.heatmap(cm_iso, annot=True, fmt='d', cmap='RdYlGn_r',\n",
                "            xticklabels=['BENIGN', 'ATTACK'], yticklabels=['BENIGN', 'ATTACK'])\n",
                "plt.xlabel('Predicted')\n",
                "plt.ylabel('Actual')\n",
                "plt.title(f'Isolation Forest Confusion Matrix (Accuracy: {iso_metrics[\"accuracy\"]*100:.2f}%)')\n",
                "plt.tight_layout()\n",
                "plt.savefig(OUTPUT_DIR / 'iso_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "print(\"‚úÖ Saved: iso_confusion_matrix.png\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Anomaly Score Distribution\n",
                "plt.figure(figsize=(12, 6))\n",
                "plt.hist(anomaly_scores[y_test_bin == 0], bins=50, alpha=0.7, label='BENIGN', color='#27ae60')\n",
                "plt.hist(anomaly_scores[y_test_bin == 1], bins=50, alpha=0.7, label='ATTACK', color='#e74c3c')\n",
                "threshold = np.percentile(anomaly_scores, 90)\n",
                "plt.axvline(x=threshold, color='#3498db', linestyle='--', lw=2, label=f'90th Percentile ({threshold:.3f})')\n",
                "plt.xlabel('Anomaly Score')\n",
                "plt.ylabel('Frequency')\n",
                "plt.title('Isolation Forest - Anomaly Score Distribution')\n",
                "plt.legend()\n",
                "plt.tight_layout()\n",
                "plt.savefig(OUTPUT_DIR / 'iso_anomaly_scores.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "print(\"‚úÖ Saved: iso_anomaly_scores.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 12. ROC Curve Comparison (Binary Classification)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train binary versions for ROC comparison\n",
                "print(\"Training binary classifiers for ROC comparison...\")\n",
                "\n",
                "binary_models = {}\n",
                "\n",
                "# Random Forest Binary\n",
                "rf_bin = RandomForestClassifier(n_estimators=100, max_depth=20, n_jobs=-1, random_state=42)\n",
                "rf_bin.fit(X_train_bin, y_train_bin)\n",
                "binary_models['Random Forest'] = rf_bin\n",
                "\n",
                "# Extra Trees Binary\n",
                "et_bin = ExtraTreesClassifier(n_estimators=100, max_depth=20, n_jobs=-1, random_state=42)\n",
                "et_bin.fit(X_train_bin, y_train_bin)\n",
                "binary_models['Extra Trees'] = et_bin\n",
                "\n",
                "if XGBClassifier is not None:\n",
                "    xgb_bin = XGBClassifier(n_estimators=100, max_depth=10, n_jobs=-1, random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
                "    xgb_bin.fit(X_train_bin, y_train_bin)\n",
                "    binary_models['XGBoost'] = xgb_bin\n",
                "\n",
                "if LGBMClassifier is not None:\n",
                "    lgbm_bin = LGBMClassifier(n_estimators=100, max_depth=10, n_jobs=-1, random_state=42, verbose=-1)\n",
                "    lgbm_bin.fit(X_train_bin, y_train_bin)\n",
                "    binary_models['LightGBM'] = lgbm_bin\n",
                "\n",
                "if CatBoostClassifier is not None:\n",
                "    cat_bin = CatBoostClassifier(iterations=100, depth=10, random_state=42, verbose=False)\n",
                "    cat_bin.fit(X_train_bin, y_train_bin)\n",
                "    binary_models['CatBoost'] = cat_bin\n",
                "\n",
                "print(f\"‚úÖ Trained {len(binary_models)} binary classifiers\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot ROC curves\n",
                "plt.figure(figsize=(10, 8))\n",
                "\n",
                "colors = ['#3498db', '#e74c3c', '#2ecc71', '#9b59b6', '#f39c12']\n",
                "\n",
                "for (name, model), color in zip(binary_models.items(), colors):\n",
                "    y_proba = model.predict_proba(X_test_bin)[:, 1]\n",
                "    fpr, tpr, _ = roc_curve(y_test_bin, y_proba)\n",
                "    roc_auc = auc(fpr, tpr)\n",
                "    plt.plot(fpr, tpr, color=color, lw=2, label=f'{name} (AUC = {roc_auc:.4f})')\n",
                "\n",
                "# Isolation Forest ROC\n",
                "fpr_iso, tpr_iso, _ = roc_curve(y_test_bin, anomaly_scores)\n",
                "roc_auc_iso = auc(fpr_iso, tpr_iso)\n",
                "plt.plot(fpr_iso, tpr_iso, color='#1abc9c', lw=2, linestyle='--', label=f'Isolation Forest (AUC = {roc_auc_iso:.4f})')\n",
                "\n",
                "# Random baseline\n",
                "plt.plot([0, 1], [0, 1], color='#95a5a6', lw=2, linestyle=':', label='Random Classifier')\n",
                "\n",
                "plt.xlim([0.0, 1.0])\n",
                "plt.ylim([0.0, 1.05])\n",
                "plt.xlabel('False Positive Rate')\n",
                "plt.ylabel('True Positive Rate')\n",
                "plt.title('ROC Curve Comparison - All Models')\n",
                "plt.legend(loc='lower right')\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.tight_layout()\n",
                "plt.savefig(OUTPUT_DIR / 'roc_comparison.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "print(\"‚úÖ Saved: roc_comparison.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 13. Model Comparison Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create comparison table\n",
                "comparison_data = []\n",
                "for name, metrics in results.items():\n",
                "    comparison_data.append({\n",
                "        'Model': name,\n",
                "        'Accuracy': f\"{metrics['accuracy']*100:.2f}%\",\n",
                "        'Precision': f\"{metrics['precision']*100:.2f}%\",\n",
                "        'Recall': f\"{metrics['recall']*100:.2f}%\",\n",
                "        'F1-Score': f\"{metrics['f1']*100:.2f}%\"\n",
                "    })\n",
                "\n",
                "comparison_df = pd.DataFrame(comparison_data)\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"MODEL COMPARISON TABLE (FOR PAPER)\")\n",
                "print(\"=\"*70)\n",
                "print(comparison_df.to_string(index=False))\n",
                "\n",
                "# Save to CSV\n",
                "comparison_df.to_csv(OUTPUT_DIR / 'model_comparison.csv', index=False)\n",
                "print(f\"\\n‚úÖ Saved: model_comparison.csv\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize comparison\n",
                "plt.figure(figsize=(12, 6))\n",
                "\n",
                "models = [d['Model'] for d in comparison_data]\n",
                "accuracies = [float(d['Accuracy'].strip('%')) for d in comparison_data]\n",
                "f1_scores = [float(d['F1-Score'].strip('%')) for d in comparison_data]\n",
                "\n",
                "x = np.arange(len(models))\n",
                "width = 0.35\n",
                "\n",
                "bars1 = plt.bar(x - width/2, accuracies, width, label='Accuracy', color='#3498db')\n",
                "bars2 = plt.bar(x + width/2, f1_scores, width, label='F1-Score', color='#e74c3c')\n",
                "\n",
                "plt.xlabel('Model')\n",
                "plt.ylabel('Score (%)')\n",
                "plt.title('Model Performance Comparison')\n",
                "plt.xticks(x, models, rotation=45, ha='right')\n",
                "plt.legend()\n",
                "plt.ylim(0, 105)\n",
                "\n",
                "# Add value labels\n",
                "for bar in bars1:\n",
                "    height = bar.get_height()\n",
                "    plt.annotate(f'{height:.1f}', xy=(bar.get_x() + bar.get_width()/2, height),\n",
                "                 xytext=(0, 3), textcoords=\"offset points\", ha='center', fontsize=9)\n",
                "for bar in bars2:\n",
                "    height = bar.get_height()\n",
                "    plt.annotate(f'{height:.1f}', xy=(bar.get_x() + bar.get_width()/2, height),\n",
                "                 xytext=(0, 3), textcoords=\"offset points\", ha='center', fontsize=9)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(OUTPUT_DIR / 'model_comparison_chart.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "print(\"‚úÖ Saved: model_comparison_chart.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 14. Save Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save all models\n",
                "print(\"Saving models...\")\n",
                "\n",
                "joblib.dump(rf_model, MODEL_DIR / 'random_forest.joblib')\n",
                "joblib.dump(et_model, MODEL_DIR / 'extra_trees.joblib')\n",
                "if XGBClassifier is not None:\n",
                "    joblib.dump(xgb_model, MODEL_DIR / 'xgboost.joblib')\n",
                "if LGBMClassifier is not None:\n",
                "    joblib.dump(lgbm_model, MODEL_DIR / 'lightgbm.joblib')\n",
                "if CatBoostClassifier is not None:\n",
                "    joblib.dump(catboost_model, MODEL_DIR / 'catboost.joblib')\n",
                "joblib.dump(iso_model, MODEL_DIR / 'isolation_forest.joblib')\n",
                "\n",
                "# Save preprocessors\n",
                "joblib.dump(scaler, MODEL_DIR / 'scaler.joblib')\n",
                "joblib.dump(label_encoder, MODEL_DIR / 'label_encoder.joblib')\n",
                "joblib.dump(numeric_cols, MODEL_DIR / 'feature_names.joblib')\n",
                "\n",
                "print(f\"‚úÖ All models saved to: {MODEL_DIR.absolute()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 15. Summary for Paper"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"SUMMARY FOR TECHNICAL PAPER\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "print(f\"\"\"\n",
                "DATASET:\n",
                "- Name: CICIDS2017\n",
                "- Total samples: {len(data):,}\n",
                "- Training samples: {len(X_train):,}\n",
                "- Test samples: {len(X_test):,}\n",
                "- Features: {len(numeric_cols)}\n",
                "- Classes: {len(class_names)}\n",
                "\n",
                "MODELS TRAINED:\n",
                "\"\"\")\n",
                "\n",
                "for name, metrics in results.items():\n",
                "    print(f\"  {name}:\")\n",
                "    print(f\"    - Accuracy:  {metrics['accuracy']*100:.2f}%\")\n",
                "    print(f\"    - F1-Score:  {metrics['f1']*100:.2f}%\")\n",
                "\n",
                "print(f\"\"\"\n",
                "SAVED FILES:\n",
                "\"\"\")\n",
                "for f in OUTPUT_DIR.glob('*.png'):\n",
                "    print(f\"  üìä {f.name}\")\n",
                "for f in OUTPUT_DIR.glob('*.csv'):\n",
                "    print(f\"  üìÑ {f.name}\")\n",
                "\n",
                "print(\"\\n‚úÖ NOTEBOOK COMPLETE!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.13.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
