\section{Performance Evaluation}

This part proposes the empirical findings obtained in our experimental assessment. We report the dataset, training scheme and performance measures of a variety of classifier structures.
\subsection{Experimental Setup}

\subsubsection{Dataset}

Our detection model was tested in the CICIDS2017 benchmark \cite{c13}, which consists of 2,830,743 labeled network flows. We have a 5 days traffic dataset containing 12 attack categories:

\begin{itemize}
    \item \textbf{DoS Variants}: Slowloris, slowhttp test, hulk, and goldeneye.
    \item \textbf{DDoS}: Distributed denial of service attacks
    \item \textbf{Reconnaissance}: PortScan, SSH Patator, FTP Patator
    \item \textbf{Web Attacks}: SQL injection, Brute Force and XSS.
    \item \textbf{Infiltration}:  Dropbox based exfiltration.
    \item \textbf{Botnet}: Ares botnet traffic
\end{itemize}

Figure~\ref{fig:attack_dist} illustrates the severe class imbalance: benign traffic constitutes 80.3\% of samples, while rare attacks (Heartbleed, Infiltration) have fewer than 100 samples each.

\begin{figure}[htbp]
\centerline{\includegraphics[width=\columnwidth]{figures/dataset_attack_distribution}}
\caption{CICIDS2017 class distribution. The 100:1 imbalance ratio between Benign and rare attack classes necessitates stratified sampling.}
\label{fig:attack_dist}
\end{figure}

\subsubsection{Training Protocol}
In order to achieve reproducibility, we used the following protocol:
\begin{enumerate}
    \item \textbf{Stratified Sampling}: 300,000 samples sampled at proportions in classes maintained.
    \item \textbf{Rare Class Filtering}: Stratification failures were avoided by dropping classes that had fewer than $10$ samples after stratification.
    \item \textbf{Train/Test Split}: 80\%/20\% stratified split (random seed = 42).
    \item \textbf{Feature Scaling}: Z-score normalization fitted on training set only.
    \item \textbf{Label Encoding}: XGBoost input, sequential encoding of XGBoost input using $\{0, 1, ..., K-1\}$.
\end{enumerate}

\subsection{Multiclass Classification Results}

Table~\ref{tab:performance} compares six classifier architectures. All supervised models were trained with consistent hyperparameters (100 estimators, max depth 10).

\begin{table}[h]
\centering
\caption{Classifier Performance Comparison (Macro-Averaged)}
\label{tab:performance}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Model} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} \\
\hline
XGBoost & \textbf{99.88\%} & \textbf{99.88\%} & \textbf{99.88\%} & \textbf{99.88\%} \\
Random Forest & 99.82\% & 99.80\% & 99.82\% & 99.81\% \\
CatBoost & 99.81\% & 99.79\% & 99.81\% & 99.79\% \\
Extra Trees & 99.13\% & 99.11\% & 99.13\% & 99.06\% \\
LightGBM & 81.36\% & 89.18\% & 81.36\% & 83.67\% \\
Isolation Forest & 81.22\% & 52.66\% & 45.22\% & 48.66\% \\
\hline
\end{tabular}
\end{table}

\textbf{Key Observations}:
\begin{itemize}
    \item Gradient boosted trees (XGBoost, CatBoost) model is between 0.06 and 0.75 percentage points better than bagging ensembles (Random Forest, Extra Trees).
    \item LightGBM performs poorly (81.36\%) due to a poor management of the re-encoded label space in post-rare-class filtering.
    \item The low accuracy of the isolated forest (52.66\%) is predictable considering that it is an unsupervised anomaly detector that is being trained on a benign traffic only.
\end{itemize}

\begin{figure}[htbp]
\centerline{\includegraphics[width=\columnwidth]{figures/model_comparison_chart}}
\caption{Comparison of performance of classifiers visually. XGBoost has the best scores in all metrics.}
\label{fig:model_comparison}
\end{figure}

\subsection{ROC Analysis}

We calculated Receiver Operating Characteristic (ROC) curves when it comes to binary classification (Attack vs. Benign). Figure~\ref{fig:roc} shows that every supervised classifier reaches AUC $> 0.99$.

\begin{figure}[htbp]
\centerline{\includegraphics[width=\columnwidth]{figures/roc_comparison}}
\caption{ROC curves of binary attack detector. There are strong class separation attested by near-perfect AUC values.}
\label{fig:roc}
\end{figure}

The Area Under Curve (AUC) scores:
\begin{align}
\text{AUC}_{\text{XGBoost}} &= 0.9997 \\
\text{AUC}_{\text{RF}} &= 0.9994 \\
\text{AUC}_{\text{CatBoost}} &= 0.9993
\end{align}

\subsection{Confusion Matrix Analysis}

The XGBoost confusion matrix (Figure~\ref{fig:xgb_cm}) discloses class performance. All the 12 categories are correctly classified by diagnosis dominance.

\begin{figure}[htbp]
\centerline{\includegraphics[width=\columnwidth]{figures/xgb_confusion_matrix}}
\caption{XGBoost confusion matrix. Off-diagonal entries indicate misclassifications between similar attack types (e.g., DoS Hulk vs. DoS GoldenEye).}
\label{fig:xgb_cm}
\end{figure}

\begin{figure}[htbp]
\centerline{\includegraphics[width=\columnwidth]{figures/rf_confusion_matrix}}
\caption{Random Forest confusion matrix for comparison. Slightly higher confusion between DoS variants.}
\label{fig:rf_cm}
\end{figure}

\subsection{Feature Importance}

We extracted Gini-based feature importance scores to identify the most discriminative predictors. Figure~\ref{fig:xgb_features} shows the top 15 features.

\begin{figure}[htbp]
\centerline{\includegraphics[width=\columnwidth]{figures/xgb_feature_importance}}
\caption{XGBoost feature importance. Destination Port and TCP window size dominate, consistent with attack signature patterns.}
\label{fig:xgb_features}
\end{figure}

The top 5 discriminative features:
\begin{enumerate}
    \item \texttt{Destination Port} (0.18): Attack-specific port targeting.
    \item \texttt{Init\_Win\_bytes\_forward} (0.14): TCP handshake anomalies.
    \item \texttt{Fwd Packet Length Max} (0.11): Payload size variations.
    \item \texttt{Flow Duration} (0.09): Attack temporal patterns.
    \item \texttt{Bwd Packet Length Mean} (0.07): Response traffic signatures.
\end{enumerate}

\subsection{System Latency Analysis}

Table~\ref{tab:latency} decomposes the end-to-end latency from packet ingress to dashboard alert.

\begin{table}[htbp]
\caption{Pipeline Latency Breakdown}
\label{tab:latency}
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Stage} & \textbf{Mean (ms)} & \textbf{P99 (ms)} \\
\midrule
Feature Extraction & 12.3 & 18.7 \\
ML Inference & 8.4 & 15.2 \\
HTTP Webhook & 23.1 & 45.6 \\
Blockchain Commit & 47.2 & 89.3 \\
SSE Broadcast & 15.8 & 32.4 \\
\midrule
\textbf{Total} & \textbf{106.8} & \textbf{201.2} \\
\bottomrule
\end{tabular}
\end{table}

The blockchain commit (47.2ms mean) represents 44\% of all the latency but offers cryptographic immutability. The P99 tail latency is 201ms which is also acceptable in real-time operations of the SOC.

\subsection{Cross-Platform Notification Latency}

Table~\ref{tab:platform} compares alert delivery across client platforms.

\begin{table}[htbp]
\caption{Cross-Platform Alert Latency}
\label{tab:platform}
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Platform} & \textbf{Mean Latency} & \textbf{Mechanism} \\
\midrule
Web (Next.js) & 1,180 ms & SSE push \\
Desktop (Electron) & 1,220 ms & SSE push \\
Mobile (React Native) & 5,340 ms & Adaptive polling \\
\bottomrule
\end{tabular}
\end{table}

Mobile latency is 4.5$\times$ higher due to iOS background execution restrictions requiring polling-based updates.

\subsection{Blockchain Throughput}

We stress-tested the Hyperledger Fabric network at various transaction rates. Figure~\ref{fig:throughput} shows that latency remains stable up to 150 TPS.

\begin{figure}[htbp]
\centerline{\includegraphics[width=\columnwidth]{figures/throughput-graph}}
\caption{Blockchain throughput vs. latency. The 2-second BatchTimeout maintains consistent block production under load.}
\label{fig:throughput}
\end{figure}

\subsection{Resource Consumption}

Table~\ref{tab:resources} presents the steady-state resource footprint under 50 Mbps traffic load.

\begin{table}[htbp]
\caption{Microservice Resource Utilization}
\label{tab:resources}
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Service} & \textbf{CPU (vCPU)} & \textbf{RAM (MB)} \\
\midrule
AI Detection (Python) & 1.4 & 450 \\
Backend API (Node.js) & 0.3 & 120 \\
Hyperledger Peer & 0.6 & 850 \\
Hyperledger Orderer & 0.2 & 300 \\
\midrule
\textbf{Total} & 2.5 & 1,720 \\
\bottomrule
\end{tabular}
\end{table}

The overall size (2.5 vCPU, 1.7 GB memory) ensures that it will work with compatible hardware (commodity t3.medium or similar).
