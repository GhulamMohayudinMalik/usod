{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Robust Binary Model Training\n",
                "\n",
                "**Goal:** Train a binary classifier (BENIGN vs ATTACK) using NFStream-extracted features.\n",
                "\n",
                "## Data Sources:\n",
                "- **BENIGN:** Monday, Tuesday, Thursday, User traffic\n",
                "- **ATTACK:** Friday (13:00-16:30 = PortScan + DDoS), Wednesday (9:43-11:24 = DoS)\n",
                "\n",
                "## Key Principle:\n",
                "Train and inference use the SAME extraction method (NFStream) = features match perfectly!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ NFStream version: 6.5.4\n",
                        "Base directory: c:\\Users\\Ghulam Mohayudin\\Projects\\Others\\usod\\network-based-ai\n",
                        "User traffic: c:\\Users\\Ghulam Mohayudin\\Projects\\Others\\usod\\network-based-ai\\user_traffic\n"
                    ]
                }
            ],
            "source": [
                "# Setup\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from pathlib import Path\n",
                "import time\n",
                "from datetime import datetime\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Check NFStream\n",
                "try:\n",
                "    from nfstream import NFStreamer\n",
                "    import nfstream\n",
                "    print(f\"‚úÖ NFStream version: {nfstream.__version__}\")\n",
                "except ImportError:\n",
                "    raise ImportError(\"NFStream not installed! Run: pip install nfstream\")\n",
                "\n",
                "# Paths\n",
                "BASE_DIR = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
                "MODELS_DIR = BASE_DIR / 'models'\n",
                "DATA_DIR = BASE_DIR / 'data_processed'\n",
                "USER_TRAFFIC_DIR = BASE_DIR / 'user_traffic'\n",
                "\n",
                "MODELS_DIR.mkdir(exist_ok=True)\n",
                "DATA_DIR.mkdir(exist_ok=True)\n",
                "\n",
                "print(f\"Base directory: {BASE_DIR}\")\n",
                "print(f\"User traffic: {USER_TRAFFIC_DIR}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## PCAP File Locations\n",
                "\n",
                "Update these paths to match your system:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "CICIDS2017 PCAP Files:\n",
                        "  ‚úÖ Monday: D:\\pcap\\Monday-WorkingHours.pcap (10.08 GB)\n",
                        "  ‚úÖ Tuesday: D:\\pcap\\Tuesday-WorkingHours.pcap (10.29 GB)\n",
                        "  ‚úÖ Wednesday: F:\\Wednesday-WorkingHours.pcap (12.50 GB)\n",
                        "  ‚úÖ Thursday: F:\\Thursday-WorkingHours.pcap (7.73 GB)\n",
                        "  ‚úÖ Friday: c:\\Users\\Ghulam Mohayudin\\Projects\\Others\\usod\\network-based-ai\\pcap\\Friday-WorkingHours.pcap (8.23 GB)\n",
                        "\n",
                        "User Traffic Files:\n",
                        "  ‚úÖ gaming_browsing_01.pcap (39.86 MB)\n",
                        "  ‚úÖ gaming_browsing_02.pcap (85.65 MB)\n",
                        "  ‚úÖ gaming_browsing_03.pcap (214.45 MB)\n",
                        "  ‚úÖ gaming_browsing_04.pcap (71.24 MB)\n",
                        "  Total: 4 files\n"
                    ]
                }
            ],
            "source": [
                "# CICIDS2017 PCAP Locations - UPDATE THESE!\n",
                "PCAP_PATHS = {\n",
                "    'Monday': Path(r'D:\\pcap\\Monday-WorkingHours.pcap'),\n",
                "    'Tuesday': Path(r'D:\\pcap\\Tuesday-WorkingHours.pcap'),\n",
                "    'Wednesday': Path(r'F:\\Wednesday-WorkingHours.pcap'),\n",
                "    'Thursday': Path(r'F:\\Thursday-WorkingHours.pcap'),\n",
                "    'Friday': BASE_DIR / 'pcap' / 'Friday-WorkingHours.pcap',\n",
                "}\n",
                "\n",
                "# Verify files exist\n",
                "print(\"CICIDS2017 PCAP Files:\")\n",
                "for day, path in PCAP_PATHS.items():\n",
                "    if path.exists():\n",
                "        size_gb = path.stat().st_size / (1024**3)\n",
                "        print(f\"  ‚úÖ {day}: {path} ({size_gb:.2f} GB)\")\n",
                "    else:\n",
                "        print(f\"  ‚ùå {day}: {path} (NOT FOUND)\")\n",
                "\n",
                "# User traffic\n",
                "print(f\"\\nUser Traffic Files:\")\n",
                "user_pcaps = list(USER_TRAFFIC_DIR.glob('*.pcap'))\n",
                "for pcap in user_pcaps:\n",
                "    size_mb = pcap.stat().st_size / (1024**2)\n",
                "    print(f\"  ‚úÖ {pcap.name} ({size_mb:.2f} MB)\")\n",
                "print(f\"  Total: {len(user_pcaps)} files\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## NFStream Feature Extraction"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Features to extract: 46\n"
                    ]
                }
            ],
            "source": [
                "# NFStream attributes to extract (46 features)\n",
                "NFSTREAM_FEATURES = [\n",
                "    'dst_port',\n",
                "    'bidirectional_duration_ms',\n",
                "    'src2dst_packets', 'dst2src_packets', 'bidirectional_packets',\n",
                "    'src2dst_bytes', 'dst2src_bytes', 'bidirectional_bytes',\n",
                "    'src2dst_max_ps', 'src2dst_min_ps', 'src2dst_mean_ps', 'src2dst_stddev_ps',\n",
                "    'dst2src_max_ps', 'dst2src_min_ps', 'dst2src_mean_ps', 'dst2src_stddev_ps',\n",
                "    'bidirectional_min_ps', 'bidirectional_max_ps', 'bidirectional_mean_ps', 'bidirectional_stddev_ps',\n",
                "    'bidirectional_mean_piat_ms', 'bidirectional_stddev_piat_ms', 'bidirectional_max_piat_ms', 'bidirectional_min_piat_ms',\n",
                "    'src2dst_duration_ms', 'src2dst_mean_piat_ms', 'src2dst_stddev_piat_ms', 'src2dst_max_piat_ms', 'src2dst_min_piat_ms',\n",
                "    'dst2src_duration_ms', 'dst2src_mean_piat_ms', 'dst2src_stddev_piat_ms', 'dst2src_max_piat_ms', 'dst2src_min_piat_ms',\n",
                "    'src2dst_psh_packets', 'src2dst_urg_packets', 'src2dst_syn_packets', 'src2dst_fin_packets', 'src2dst_rst_packets', 'src2dst_ack_packets',\n",
                "    'dst2src_psh_packets', 'dst2src_urg_packets', 'dst2src_syn_packets', 'dst2src_fin_packets', 'dst2src_rst_packets', 'dst2src_ack_packets',\n",
                "]\n",
                "\n",
                "print(f\"Features to extract: {len(NFSTREAM_FEATURES)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "def extract_flows(pcap_path, max_flows=250000, label=None, skip_flows=0):\n",
                "    \"\"\"\n",
                "    Extract NFStream features from PCAP.\n",
                "    \n",
                "    Args:\n",
                "        pcap_path: Path to PCAP file\n",
                "        max_flows: Max flows to extract\n",
                "        label: Label to assign ('BENIGN' or 'ATTACK')\n",
                "        skip_flows: Number of initial flows to skip\n",
                "    \"\"\"\n",
                "    print(f\"\\n{'='*60}\")\n",
                "    print(f\"Extracting from: {Path(pcap_path).name}\")\n",
                "    print(f\"Label: {label} | Max: {max_flows:,} | Skip: {skip_flows:,}\")\n",
                "    print(f\"{'='*60}\")\n",
                "    \n",
                "    streamer = NFStreamer(\n",
                "        source=str(pcap_path),\n",
                "        statistical_analysis=True,\n",
                "        splt_analysis=0,\n",
                "        n_dissections=0,\n",
                "    )\n",
                "    \n",
                "    flows_list = []\n",
                "    start_time = time.time()\n",
                "    skipped = 0\n",
                "    \n",
                "    for i, flow in enumerate(streamer):\n",
                "        # Skip initial flows if requested\n",
                "        if skipped < skip_flows:\n",
                "            skipped += 1\n",
                "            continue\n",
                "        \n",
                "        # Extract features\n",
                "        flow_dict = {}\n",
                "        for attr in NFSTREAM_FEATURES:\n",
                "            try:\n",
                "                value = getattr(flow, attr, 0)\n",
                "                flow_dict[attr] = 0 if value is None else value\n",
                "            except:\n",
                "                flow_dict[attr] = 0\n",
                "        \n",
                "        if label:\n",
                "            flow_dict['label'] = label\n",
                "        \n",
                "        flows_list.append(flow_dict)\n",
                "        \n",
                "        if len(flows_list) % 50000 == 0:\n",
                "            elapsed = time.time() - start_time\n",
                "            rate = len(flows_list) / elapsed if elapsed > 0 else 0\n",
                "            print(f\"  Extracted {len(flows_list):,} flows... ({rate:.0f}/sec)\")\n",
                "        \n",
                "        if len(flows_list) >= max_flows:\n",
                "            break\n",
                "    \n",
                "    elapsed = time.time() - start_time\n",
                "    print(f\"‚úÖ Done! {len(flows_list):,} flows in {elapsed:.1f}s\")\n",
                "    \n",
                "    return pd.DataFrame(flows_list)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Extract BENIGN Traffic\n",
                "\n",
                "Sources:\n",
                "- Monday (all benign)\n",
                "- Tuesday (97% benign)\n",
                "- Thursday (98% benign)\n",
                "- User traffic (100% benign)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "============================================================\n",
                        "Extracting from: Monday-WorkingHours.pcap\n",
                        "Label: BENIGN | Max: 200,000 | Skip: 0\n",
                        "============================================================\n",
                        "  Extracted 50,000 flows... (245/sec)\n",
                        "  Extracted 100,000 flows... (465/sec)\n",
                        "  Extracted 150,000 flows... (657/sec)\n",
                        "  Extracted 200,000 flows... (834/sec)\n",
                        "‚úÖ Done! 200,000 flows in 239.7s\n",
                        "Monday: 200,000 flows\n"
                    ]
                }
            ],
            "source": [
                "# Extract BENIGN from Monday\n",
                "df_monday = extract_flows(PCAP_PATHS['Monday'], max_flows=200000, label='BENIGN')\n",
                "print(f\"Monday: {len(df_monday):,} flows\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "============================================================\n",
                        "Extracting from: Tuesday-WorkingHours.pcap\n",
                        "Label: BENIGN | Max: 150,000 | Skip: 0\n",
                        "============================================================\n",
                        "  Extracted 50,000 flows... (232/sec)\n",
                        "  Extracted 100,000 flows... (431/sec)\n",
                        "  Extracted 150,000 flows... (611/sec)\n",
                        "‚úÖ Done! 150,000 flows in 245.6s\n",
                        "Tuesday: 150,000 flows\n"
                    ]
                }
            ],
            "source": [
                "# Extract BENIGN from Tuesday\n",
                "df_tuesday = extract_flows(PCAP_PATHS['Tuesday'], max_flows=150000, label='BENIGN')\n",
                "print(f\"Tuesday: {len(df_tuesday):,} flows\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "============================================================\n",
                        "Extracting from: Thursday-WorkingHours.pcap\n",
                        "Label: BENIGN | Max: 150,000 | Skip: 0\n",
                        "============================================================\n",
                        "  Extracted 50,000 flows... (1319/sec)\n",
                        "  Extracted 100,000 flows... (2070/sec)\n",
                        "  Extracted 150,000 flows... (2774/sec)\n",
                        "‚úÖ Done! 150,000 flows in 54.1s\n",
                        "Thursday: 150,000 flows\n"
                    ]
                }
            ],
            "source": [
                "# Extract BENIGN from Thursday\n",
                "df_thursday = extract_flows(PCAP_PATHS['Thursday'], max_flows=150000, label='BENIGN')\n",
                "print(f\"Thursday: {len(df_thursday):,} flows\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "============================================================\n",
                        "Extracting from: gaming_browsing_01.pcap\n",
                        "Label: BENIGN | Max: 500,000 | Skip: 0\n",
                        "============================================================\n",
                        "‚úÖ Done! 959 flows in 2.2s\n",
                        "\n",
                        "============================================================\n",
                        "Extracting from: gaming_browsing_02.pcap\n",
                        "Label: BENIGN | Max: 500,000 | Skip: 0\n",
                        "============================================================\n",
                        "‚úÖ Done! 343 flows in 3.1s\n",
                        "\n",
                        "============================================================\n",
                        "Extracting from: gaming_browsing_03.pcap\n",
                        "Label: BENIGN | Max: 500,000 | Skip: 0\n",
                        "============================================================\n",
                        "‚úÖ Done! 1,957 flows in 4.7s\n",
                        "\n",
                        "============================================================\n",
                        "Extracting from: gaming_browsing_04.pcap\n",
                        "Label: BENIGN | Max: 500,000 | Skip: 0\n",
                        "============================================================\n",
                        "‚úÖ Done! 563 flows in 2.7s\n",
                        "\n",
                        "üìä Total User Traffic: 3,822 flows\n"
                    ]
                }
            ],
            "source": [
                "# Extract BENIGN from User Traffic (ALL files)\n",
                "user_dfs = []\n",
                "for pcap in user_pcaps:\n",
                "    df = extract_flows(pcap, max_flows=500000, label='BENIGN')  # Extract all\n",
                "    user_dfs.append(df)\n",
                "\n",
                "df_user = pd.concat(user_dfs, ignore_index=True) if user_dfs else pd.DataFrame()\n",
                "print(f\"\\nüìä Total User Traffic: {len(df_user):,} flows\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "‚úÖ TOTAL BENIGN: 503,822 flows\n"
                    ]
                }
            ],
            "source": [
                "# Combine all BENIGN\n",
                "df_benign = pd.concat([df_monday, df_tuesday, df_thursday, df_user], ignore_index=True)\n",
                "print(f\"\\n‚úÖ TOTAL BENIGN: {len(df_benign):,} flows\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Extract ATTACK Traffic\n",
                "\n",
                "Sources:\n",
                "- **Friday (after 13:00)**: PortScan (13:00-15:28) + DDoS (15:51-16:21)\n",
                "- **Wednesday (9:43-11:24)**: DoS attacks\n",
                "\n",
                "**Strategy**: Skip early flows to get to attack time window."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "============================================================\n",
                        "Extracting from: Friday-WorkingHours.pcap\n",
                        "Label: ATTACK | Max: 200,000 | Skip: 250,000\n",
                        "============================================================\n",
                        "  Extracted 50,000 flows... (999/sec)\n",
                        "  Extracted 100,000 flows... (1847/sec)\n",
                        "  Extracted 150,000 flows... (2582/sec)\n",
                        "  Extracted 200,000 flows... (3136/sec)\n",
                        "‚úÖ Done! 200,000 flows in 63.8s\n",
                        "Friday ATTACK: 200,000 flows\n"
                    ]
                }
            ],
            "source": [
                "# Friday: Skip first ~250K flows to get to afternoon attacks (PortScan + DDoS)\n",
                "# The Friday PCAP starts in the morning, attacks begin around 13:00\n",
                "# Skipping 250K gets us past the morning benign traffic\n",
                "\n",
                "df_friday_attack = extract_flows(\n",
                "    PCAP_PATHS['Friday'], \n",
                "    max_flows=200000, \n",
                "    label='ATTACK',\n",
                "    skip_flows=250000  # Skip morning benign traffic\n",
                ")\n",
                "print(f\"Friday ATTACK: {len(df_friday_attack):,} flows\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "============================================================\n",
                        "Extracting from: Wednesday-WorkingHours.pcap\n",
                        "Label: ATTACK | Max: 150,000 | Skip: 0\n",
                        "============================================================\n",
                        "  Extracted 50,000 flows... (796/sec)\n",
                        "  Extracted 100,000 flows... (1152/sec)\n",
                        "  Extracted 150,000 flows... (1608/sec)\n",
                        "‚úÖ Done! 150,000 flows in 93.3s\n",
                        "Wednesday ATTACK: 150,000 flows\n"
                    ]
                }
            ],
            "source": [
                "# Wednesday: DoS attacks are in morning (9:43-11:24)\n",
                "# This is near the start of the PCAP, so we don't skip\n",
                "# But we limit extraction to ~150K flows to stay in attack window\n",
                "\n",
                "df_wednesday_attack = extract_flows(\n",
                "    PCAP_PATHS['Wednesday'], \n",
                "    max_flows=150000, \n",
                "    label='ATTACK',\n",
                "    skip_flows=0  # Attacks are at start\n",
                ")\n",
                "print(f\"Wednesday ATTACK: {len(df_wednesday_attack):,} flows\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "‚úÖ TOTAL ATTACK: 350,000 flows\n"
                    ]
                }
            ],
            "source": [
                "# Combine all ATTACK\n",
                "df_attack = pd.concat([df_friday_attack, df_wednesday_attack], ignore_index=True)\n",
                "print(f\"\\n‚úÖ TOTAL ATTACK: {len(df_attack):,} flows\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Combine & Balance Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Combining datasets...\n",
                        "\n",
                        "Combined dataset: 853,822 flows\n",
                        "\n",
                        "Label distribution:\n",
                        "label\n",
                        "BENIGN    503822\n",
                        "ATTACK    350000\n",
                        "Name: count, dtype: int64\n"
                    ]
                }
            ],
            "source": [
                "# Combine datasets\n",
                "print(\"Combining datasets...\")\n",
                "df_combined = pd.concat([df_benign, df_attack], ignore_index=True)\n",
                "\n",
                "print(f\"\\nCombined dataset: {len(df_combined):,} flows\")\n",
                "print(f\"\\nLabel distribution:\")\n",
                "print(df_combined['label'].value_counts())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Balancing to 350,000 samples per class...\n",
                        "\n",
                        "‚úÖ Balanced dataset: 700,000 flows\n",
                        "label\n",
                        "ATTACK    350000\n",
                        "BENIGN    350000\n",
                        "Name: count, dtype: int64\n"
                    ]
                }
            ],
            "source": [
                "# Balance classes (undersample majority)\n",
                "benign_count = (df_combined['label'] == 'BENIGN').sum()\n",
                "attack_count = (df_combined['label'] == 'ATTACK').sum()\n",
                "min_count = min(benign_count, attack_count)\n",
                "\n",
                "print(f\"Balancing to {min_count:,} samples per class...\")\n",
                "\n",
                "df_benign_sample = df_combined[df_combined['label'] == 'BENIGN'].sample(n=min_count, random_state=42)\n",
                "df_attack_sample = df_combined[df_combined['label'] == 'ATTACK'].sample(n=min_count, random_state=42)\n",
                "\n",
                "df_balanced = pd.concat([df_benign_sample, df_attack_sample], ignore_index=True)\n",
                "df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)  # Shuffle\n",
                "\n",
                "print(f\"\\n‚úÖ Balanced dataset: {len(df_balanced):,} flows\")\n",
                "print(df_balanced['label'].value_counts())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üíæ Saved to: c:\\Users\\Ghulam Mohayudin\\Projects\\Others\\usod\\network-based-ai\\data_processed\\nfstream_robust_binary_training.csv\n"
                    ]
                }
            ],
            "source": [
                "# Save combined dataset\n",
                "combined_path = DATA_DIR / 'nfstream_robust_binary_training.csv'\n",
                "df_balanced.to_csv(combined_path, index=False)\n",
                "print(f\"üíæ Saved to: {combined_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Train Random Forest Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Features shape: (700000, 46)\n",
                        "Labels shape: (700000,)\n"
                    ]
                }
            ],
            "source": [
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
                "import joblib\n",
                "\n",
                "# Prepare features and labels\n",
                "X = df_balanced[NFSTREAM_FEATURES].copy()\n",
                "y = df_balanced['label']\n",
                "\n",
                "# Handle inf/nan\n",
                "X = X.replace([np.inf, -np.inf], np.nan)\n",
                "X = X.fillna(0)\n",
                "\n",
                "print(f\"Features shape: {X.shape}\")\n",
                "print(f\"Labels shape: {y.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Training set: 560,000\n",
                        "Test set: 140,000\n"
                    ]
                }
            ],
            "source": [
                "# Train-test split\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y, test_size=0.2, random_state=42, stratify=y\n",
                ")\n",
                "\n",
                "print(f\"Training set: {len(X_train):,}\")\n",
                "print(f\"Test set: {len(X_test):,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Training Random Forest...\n",
                        "============================================================\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
                        "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   16.4s\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "‚úÖ Training done in 76.8s\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:  1.3min finished\n"
                    ]
                }
            ],
            "source": [
                "# Train model\n",
                "print(\"Training Random Forest...\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "model = RandomForestClassifier(\n",
                "    n_estimators=150,\n",
                "    max_depth=30,\n",
                "    min_samples_split=5,\n",
                "    class_weight='balanced',\n",
                "    random_state=42,\n",
                "    n_jobs=-1,\n",
                "    verbose=1\n",
                ")\n",
                "\n",
                "start = time.time()\n",
                "model.fit(X_train, y_train)\n",
                "print(f\"\\n‚úÖ Training done in {time.time()-start:.1f}s\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5: Evaluate Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
                        "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.1s\n",
                        "[Parallel(n_jobs=12)]: Done 150 out of 150 | elapsed:    0.9s finished\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "============================================================\n",
                        "MODEL PERFORMANCE\n",
                        "============================================================\n",
                        "\n",
                        "Accuracy: 0.7710 (77.10%)\n",
                        "\n",
                        "Classification Report:\n",
                        "              precision    recall  f1-score   support\n",
                        "\n",
                        "      ATTACK       0.90      0.61      0.73     70000\n",
                        "      BENIGN       0.70      0.94      0.80     70000\n",
                        "\n",
                        "    accuracy                           0.77    140000\n",
                        "   macro avg       0.80      0.77      0.76    140000\n",
                        "weighted avg       0.80      0.77      0.76    140000\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Predictions\n",
                "y_pred = model.predict(X_test)\n",
                "\n",
                "# Metrics\n",
                "accuracy = accuracy_score(y_test, y_pred)\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"MODEL PERFORMANCE\")\n",
                "print(\"=\"*60)\n",
                "print(f\"\\nAccuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
                "print(\"\\nClassification Report:\")\n",
                "print(classification_report(y_test, y_pred))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Confusion Matrix:\n",
                        "                 Predicted\n",
                        "                 BENIGN  ATTACK\n",
                        "Actual BENIGN     65531    4469\n",
                        "       ATTACK     27587   42413\n",
                        "\n",
                        "==================================================\n",
                        "FALSE POSITIVE RATE: 0.0638 (6.38%)\n",
                        "FALSE NEGATIVE RATE: 0.3941 (39.41%)\n",
                        "==================================================\n",
                        "\n",
                        "‚ö†Ô∏è FPR 5-10% - Good\n"
                    ]
                }
            ],
            "source": [
                "# Confusion Matrix\n",
                "cm = confusion_matrix(y_test, y_pred, labels=['BENIGN', 'ATTACK'])\n",
                "tn, fp, fn, tp = cm[0,0], cm[0,1], cm[1,0], cm[1,1]\n",
                "\n",
                "print(\"Confusion Matrix:\")\n",
                "print(f\"                 Predicted\")\n",
                "print(f\"                 BENIGN  ATTACK\")\n",
                "print(f\"Actual BENIGN    {tn:>6}  {fp:>6}\")\n",
                "print(f\"       ATTACK    {fn:>6}  {tp:>6}\")\n",
                "\n",
                "# Key metrics\n",
                "fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
                "fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
                "\n",
                "print(f\"\\n\" + \"=\"*50)\n",
                "print(f\"FALSE POSITIVE RATE: {fpr:.4f} ({fpr*100:.2f}%)\")\n",
                "print(f\"FALSE NEGATIVE RATE: {fnr:.4f} ({fnr*100:.2f}%)\")\n",
                "print(\"=\"*50)\n",
                "\n",
                "if fpr < 0.05:\n",
                "    print(\"\\n‚úÖ FPR < 5% - Excellent!\")\n",
                "elif fpr < 0.10:\n",
                "    print(\"\\n‚ö†Ô∏è FPR 5-10% - Good\")\n",
                "else:\n",
                "    print(\"\\n‚ùå FPR > 10% - Needs improvement\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 6: Save Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Model saved: c:\\Users\\Ghulam Mohayudin\\Projects\\Others\\usod\\network-based-ai\\models\\random_forest_nfstream_robust_binary.joblib\n",
                        "‚úÖ Features saved: c:\\Users\\Ghulam Mohayudin\\Projects\\Others\\usod\\network-based-ai\\models\\feature_names_nfstream_robust_binary.joblib\n",
                        "‚úÖ Classes saved: c:\\Users\\Ghulam Mohayudin\\Projects\\Others\\usod\\network-based-ai\\models\\class_names_nfstream_robust_binary.joblib\n",
                        "‚úÖ Info saved: c:\\Users\\Ghulam Mohayudin\\Projects\\Others\\usod\\network-based-ai\\models\\random_forest_nfstream_robust_binary_info.txt\n"
                    ]
                }
            ],
            "source": [
                "# Save model\n",
                "model_name = 'nfstream_robust_binary'\n",
                "\n",
                "model_path = MODELS_DIR / f'random_forest_{model_name}.joblib'\n",
                "joblib.dump(model, model_path)\n",
                "print(f\"‚úÖ Model saved: {model_path}\")\n",
                "\n",
                "# Save features\n",
                "features_path = MODELS_DIR / f'feature_names_{model_name}.joblib'\n",
                "joblib.dump(NFSTREAM_FEATURES, features_path)\n",
                "print(f\"‚úÖ Features saved: {features_path}\")\n",
                "\n",
                "# Save classes\n",
                "classes_path = MODELS_DIR / f'class_names_{model_name}.joblib'\n",
                "joblib.dump(['BENIGN', 'ATTACK'], classes_path)\n",
                "print(f\"‚úÖ Classes saved: {classes_path}\")\n",
                "\n",
                "# Save info\n",
                "info_path = MODELS_DIR / f'random_forest_{model_name}_info.txt'\n",
                "with open(info_path, 'w') as f:\n",
                "    f.write(f\"Model: NFStream Robust Binary\\n\")\n",
                "    f.write(f\"Trained: {datetime.now().strftime('%Y-%m-%d %H:%M')}\\n\")\n",
                "    f.write(f\"Accuracy: {accuracy:.4f}\\n\")\n",
                "    f.write(f\"FPR: {fpr:.4f}\\n\")\n",
                "    f.write(f\"Training data: CICIDS2017 + User traffic\\n\")\n",
                "print(f\"‚úÖ Info saved: {info_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 7: Quick Test on Friday PCAP"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Testing on held-out Friday PCAP data...\n",
                        "\n",
                        "============================================================\n",
                        "Extracting from: Friday-WorkingHours.pcap\n",
                        "Label: None | Max: 50,000 | Skip: 450,000\n",
                        "============================================================\n",
                        "  Extracted 50,000 flows... (740/sec)\n",
                        "‚úÖ Done! 50,000 flows in 67.6s\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
                        "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "============================================================\n",
                        "HELD-OUT FRIDAY TEST RESULTS\n",
                        "============================================================\n",
                        "ATTACK: 31,010 (62.0%)\n",
                        "BENIGN: 18,990 (38.0%)\n",
                        "\n",
                        "‚úÖ Model correctly detects attacks in Friday PCAP!\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[Parallel(n_jobs=12)]: Done 150 out of 150 | elapsed:    0.2s finished\n"
                    ]
                }
            ],
            "source": [
                "# Test on held-out Friday data\n",
                "print(\"Testing on held-out Friday PCAP data...\")\n",
                "\n",
                "# Extract flows we haven't used (skip first 450K)\n",
                "df_test = extract_flows(\n",
                "    PCAP_PATHS['Friday'],\n",
                "    max_flows=50000,\n",
                "    skip_flows=450000\n",
                ")\n",
                "\n",
                "X_friday = df_test[NFSTREAM_FEATURES].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
                "predictions = model.predict(X_friday)\n",
                "\n",
                "# Results\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"HELD-OUT FRIDAY TEST RESULTS\")\n",
                "print(\"=\"*60)\n",
                "attack_count = (predictions == 'ATTACK').sum()\n",
                "benign_count = (predictions == 'BENIGN').sum()\n",
                "print(f\"ATTACK: {attack_count:,} ({attack_count/len(predictions)*100:.1f}%)\")\n",
                "print(f\"BENIGN: {benign_count:,} ({benign_count/len(predictions)*100:.1f}%)\")\n",
                "\n",
                "if attack_count > benign_count:\n",
                "    print(\"\\n‚úÖ Model correctly detects attacks in Friday PCAP!\")\n",
                "else:\n",
                "    print(\"\\n‚ö†Ô∏è Model may need tuning\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Done!\n",
                "\n",
                "Model saved to `models/random_forest_nfstream_robust_binary.joblib`\n",
                "\n",
                "To use:\n",
                "```python\n",
                "from src.analyzer import NetworkThreatAnalyzer\n",
                "analyzer = NetworkThreatAnalyzer()\n",
                "results = analyzer.analyze_pcap('your_file.pcap', model_type='nfstream')\n",
                "```"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
