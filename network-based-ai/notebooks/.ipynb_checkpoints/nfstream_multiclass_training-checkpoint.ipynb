{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NFStream Multiclass Model Training\n",
    "\n",
    "**Goal:** Train multiclass Random Forest directly on ALL 5 PCAP files using NFStream.\n",
    "\n",
    "## Attack Classes (8):\n",
    "- BENIGN\n",
    "- Brute Force (FTP-Patator, SSH-Patator)\n",
    "- DoS (Slowloris, Slowhttp, Hulk, GoldenEye)\n",
    "- Heartbleed\n",
    "- Web Attack (Brute Force, XSS, SQL Injection)\n",
    "- Infiltration\n",
    "- Bot\n",
    "- PortScan\n",
    "- DDoS\n",
    "\n",
    "## PCAP Locations:\n",
    "| Day | Location |\n",
    "|-----|----------|\n",
    "| Monday | D:\\ |\n",
    "| Tuesday | D:\\ |\n",
    "| Wednesday | F:\\ |\n",
    "| Thursday | F:\\ |\n",
    "| Friday | pcap/ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime, time\n",
    "import warnings\n",
    "import gc\n",
    "import joblib\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NFStream\n",
    "try:\n",
    "    from nfstream import NFStreamer\n",
    "    print(f\"✅ NFStream ready\")\n",
    "except ImportError:\n",
    "    raise ImportError(\"NFStream not installed! Run: pip install nfstream\")\n",
    "\n",
    "# Paths\n",
    "BASE_DIR = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "MODELS_DIR = BASE_DIR / 'models'\n",
    "DATA_DIR = BASE_DIR / 'data_processed'\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Base: {BASE_DIR}\")\n",
    "print(f\"Models: {MODELS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCAP File Paths - UPDATE THESE IF NEEDED\n",
    "PCAP_FILES = {\n",
    "    'Monday': Path('D:/Monday-WorkingHours.pcap'),\n",
    "    'Tuesday': Path('D:/Tuesday-WorkingHours.pcap'),\n",
    "    'Wednesday': Path('F:/Wednesday-WorkingHours.pcap'),\n",
    "    'Thursday': Path('F:/Thursday-WorkingHours.pcap'),\n",
    "    'Friday': BASE_DIR / 'pcap' / 'Friday-WorkingHours.pcap'\n",
    "}\n",
    "\n",
    "# Verify files exist\n",
    "print(\"PCAP Files:\")\n",
    "for day, path in PCAP_FILES.items():\n",
    "    if path.exists():\n",
    "        size_gb = path.stat().st_size / (1024**3)\n",
    "        print(f\"  ✅ {day}: {path} ({size_gb:.2f} GB)\")\n",
    "    else:\n",
    "        print(f\"  ❌ {day}: {path} NOT FOUND\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NFStream features to extract (46 features - optimized)\n",
    "NFSTREAM_FEATURES = [\n",
    "    'dst_port',\n",
    "    'bidirectional_duration_ms',\n",
    "    'src2dst_packets', 'dst2src_packets', 'bidirectional_packets',\n",
    "    'src2dst_bytes', 'dst2src_bytes', 'bidirectional_bytes',\n",
    "    'src2dst_max_ps', 'src2dst_min_ps', 'src2dst_mean_ps', 'src2dst_stddev_ps',\n",
    "    'dst2src_max_ps', 'dst2src_min_ps', 'dst2src_mean_ps', 'dst2src_stddev_ps',\n",
    "    'bidirectional_min_ps', 'bidirectional_max_ps', 'bidirectional_mean_ps', 'bidirectional_stddev_ps',\n",
    "    'bidirectional_mean_piat_ms', 'bidirectional_stddev_piat_ms', 'bidirectional_max_piat_ms', 'bidirectional_min_piat_ms',\n",
    "    'src2dst_duration_ms', 'src2dst_mean_piat_ms', 'src2dst_stddev_piat_ms', 'src2dst_max_piat_ms', 'src2dst_min_piat_ms',\n",
    "    'dst2src_duration_ms', 'dst2src_mean_piat_ms', 'dst2src_stddev_piat_ms', 'dst2src_max_piat_ms', 'dst2src_min_piat_ms',\n",
    "    'src2dst_psh_packets', 'src2dst_urg_packets', 'src2dst_syn_packets', 'src2dst_fin_packets', 'src2dst_rst_packets', 'src2dst_ack_packets',\n",
    "    'dst2src_psh_packets', 'dst2src_urg_packets', 'dst2src_syn_packets', 'dst2src_fin_packets', 'dst2src_rst_packets', 'dst2src_ack_packets',\n",
    "]\n",
    "\n",
    "print(f\"Features: {len(NFSTREAM_FEATURES)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attack time schedules (CICIDS2017)\n",
    "# Format: (start_hour, start_min, end_hour, end_min, label)\n",
    "\n",
    "ATTACK_SCHEDULES = {\n",
    "    'Monday': [],  # All BENIGN\n",
    "    \n",
    "    'Tuesday': [\n",
    "        (9, 17, 10, 30, 'Brute Force'),   # FTP-Patator\n",
    "        (13, 0, 16, 11, 'Brute Force'),   # SSH-Patator\n",
    "    ],\n",
    "    \n",
    "    'Wednesday': [\n",
    "        (9, 47, 10, 12, 'DoS'),      # Slowloris\n",
    "        (10, 13, 10, 38, 'DoS'),     # Slowhttptest\n",
    "        (10, 39, 11, 9, 'DoS'),      # Hulk\n",
    "        (11, 10, 11, 23, 'DoS'),     # GoldenEye\n",
    "        (15, 11, 15, 33, 'Heartbleed'),\n",
    "    ],\n",
    "    \n",
    "    'Thursday': [\n",
    "        (9, 10, 10, 12, 'Web Attack'),    # Brute Force\n",
    "        (10, 13, 10, 37, 'Web Attack'),   # XSS\n",
    "        (10, 39, 10, 45, 'Web Attack'),   # SQL Injection\n",
    "        (14, 15, 15, 50, 'Infiltration'),\n",
    "    ],\n",
    "    \n",
    "    'Friday': [\n",
    "        (9, 30, 12, 59, 'Bot'),\n",
    "        (12, 30, 15, 40, 'PortScan'),\n",
    "        (15, 40, 16, 30, 'DDoS'),\n",
    "    ],\n",
    "}\n",
    "\n",
    "def get_label_for_time(day: str, flow_time_ms: int) -> str:\n",
    "    \"\"\"Determine label based on flow timestamp.\"\"\"\n",
    "    # Convert milliseconds to time of day\n",
    "    # Assuming flow_time_ms is ms since epoch\n",
    "    try:\n",
    "        dt = datetime.fromtimestamp(flow_time_ms / 1000)\n",
    "        flow_hour = dt.hour\n",
    "        flow_min = dt.minute\n",
    "    except:\n",
    "        return 'BENIGN'\n",
    "    \n",
    "    schedules = ATTACK_SCHEDULES.get(day, [])\n",
    "    for start_h, start_m, end_h, end_m, label in schedules:\n",
    "        start_mins = start_h * 60 + start_m\n",
    "        end_mins = end_h * 60 + end_m\n",
    "        flow_mins = flow_hour * 60 + flow_min\n",
    "        \n",
    "        if start_mins <= flow_mins <= end_mins:\n",
    "            return label\n",
    "    \n",
    "    return 'BENIGN'\n",
    "\n",
    "print(\"Attack schedules configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Memory-Efficient Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_chunked(pcap_path: Path, day: str, \n",
    "                             max_flows: int = 100000,\n",
    "                             chunk_size: int = 25000) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract features from PCAP in chunks for memory efficiency.\n",
    "    Samples evenly across the PCAP to get diverse traffic patterns.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Extracting: {pcap_path.name}\")\n",
    "    print(f\"Max flows: {max_flows:,}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    if not pcap_path.exists():\n",
    "        print(f\"  ❌ File not found, skipping\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    streamer = NFStreamer(\n",
    "        source=str(pcap_path),\n",
    "        statistical_analysis=True,\n",
    "        splt_analysis=0,\n",
    "        n_dissections=0,\n",
    "    )\n",
    "    \n",
    "    flows_list = []\n",
    "    flow_count = 0\n",
    "    label_counts = {}\n",
    "    \n",
    "    import time as time_module\n",
    "    start_time = time_module.time()\n",
    "    \n",
    "    for flow in streamer:\n",
    "        # Extract features\n",
    "        flow_dict = {}\n",
    "        for attr in NFSTREAM_FEATURES:\n",
    "            try:\n",
    "                value = getattr(flow, attr, 0)\n",
    "                # Use float32 to save memory\n",
    "                flow_dict[attr] = np.float32(value) if value is not None else np.float32(0)\n",
    "            except:\n",
    "                flow_dict[attr] = np.float32(0)\n",
    "        \n",
    "        # Get timestamp for labeling\n",
    "        flow_time = getattr(flow, 'bidirectional_first_seen_ms', 0)\n",
    "        label = get_label_for_time(day, flow_time)\n",
    "        flow_dict['Label'] = label\n",
    "        \n",
    "        flows_list.append(flow_dict)\n",
    "        flow_count += 1\n",
    "        \n",
    "        # Track labels\n",
    "        label_counts[label] = label_counts.get(label, 0) + 1\n",
    "        \n",
    "        if flow_count % chunk_size == 0:\n",
    "            elapsed = time_module.time() - start_time\n",
    "            rate = flow_count / elapsed if elapsed > 0 else 0\n",
    "            print(f\"  Processed {flow_count:,} flows ({rate:.0f}/sec)\")\n",
    "            gc.collect()  # Free memory\n",
    "        \n",
    "        if flow_count >= max_flows:\n",
    "            break\n",
    "    \n",
    "    elapsed = time_module.time() - start_time\n",
    "    print(f\"\\n✅ Extracted {flow_count:,} flows in {elapsed:.1f}s\")\n",
    "    print(f\"Label distribution: {label_counts}\")\n",
    "    \n",
    "    # Convert to DataFrame with optimized dtypes\n",
    "    df = pd.DataFrame(flows_list)\n",
    "    \n",
    "    # Optimize memory\n",
    "    for col in df.columns:\n",
    "        if col != 'Label' and df[col].dtype == 'float64':\n",
    "            df[col] = df[col].astype('float32')\n",
    "    \n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "print(\"Extraction function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Extract Features from All PCAPs\n",
    "\n",
    "**Run each cell one at a time** to control memory and monitor progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Monday (BENIGN only)\n",
    "# Sample less since it's all benign\n",
    "df_monday = extract_features_chunked(PCAP_FILES['Monday'], 'Monday', max_flows=80000)\n",
    "print(f\"\\nMonday shape: {df_monday.shape}\")\n",
    "print(f\"Memory: {df_monday.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Tuesday (Brute Force attacks)\n",
    "df_tuesday = extract_features_chunked(PCAP_FILES['Tuesday'], 'Tuesday', max_flows=100000)\n",
    "print(f\"\\nTuesday shape: {df_tuesday.shape}\")\n",
    "print(f\"Memory: {df_tuesday.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Wednesday (DoS attacks)\n",
    "df_wednesday = extract_features_chunked(PCAP_FILES['Wednesday'], 'Wednesday', max_flows=100000)\n",
    "print(f\"\\nWednesday shape: {df_wednesday.shape}\")\n",
    "print(f\"Memory: {df_wednesday.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Thursday (Web Attack, Infiltration)\n",
    "df_thursday = extract_features_chunked(PCAP_FILES['Thursday'], 'Thursday', max_flows=100000)\n",
    "print(f\"\\nThursday shape: {df_thursday.shape}\")\n",
    "print(f\"Memory: {df_thursday.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Friday (Bot, PortScan, DDoS)\n",
    "df_friday = extract_features_chunked(PCAP_FILES['Friday'], 'Friday', max_flows=150000)\n",
    "print(f\"\\nFriday shape: {df_friday.shape}\")\n",
    "print(f\"Memory: {df_friday.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save individual DataFrames to disk (optional - for recovery)\n",
    "for day, df in [('monday', df_monday), ('tuesday', df_tuesday), \n",
    "                ('wednesday', df_wednesday), ('thursday', df_thursday), ('friday', df_friday)]:\n",
    "    if len(df) > 0:\n",
    "        path = DATA_DIR / f'nfstream_{day}_multiclass.csv'\n",
    "        df.to_csv(path, index=False)\n",
    "        print(f\"Saved: {path.name} ({len(df):,} rows)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Combine and Balance Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all days\n",
    "all_dfs = []\n",
    "for name, df in [('Monday', df_monday), ('Tuesday', df_tuesday), \n",
    "                 ('Wednesday', df_wednesday), ('Thursday', df_thursday), ('Friday', df_friday)]:\n",
    "    if len(df) > 0:\n",
    "        print(f\"{name}: {len(df):,} flows\")\n",
    "        all_dfs.append(df)\n",
    "\n",
    "df_combined = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "# Free memory\n",
    "del df_monday, df_tuesday, df_wednesday, df_thursday, df_friday\n",
    "gc.collect()\n",
    "\n",
    "print(f\"\\nCombined: {len(df_combined):,} flows\")\n",
    "print(f\"Memory: {df_combined.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check label distribution\n",
    "print(\"Label Distribution:\")\n",
    "label_counts = df_combined['Label'].value_counts()\n",
    "for label, count in label_counts.items():\n",
    "    pct = count / len(df_combined) * 100\n",
    "    print(f\"  {label}: {count:,} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance classes by undersampling majority class\n",
    "# This ensures the model learns all attack types\n",
    "\n",
    "# Get minimum class count (but cap at reasonable level)\n",
    "min_count = max(label_counts.min(), 1000)  # At least 1000 samples\n",
    "target_per_class = min(min_count * 3, 50000)  # Cap at 50k per class\n",
    "\n",
    "print(f\"\\nBalancing to ~{target_per_class:,} samples per class...\")\n",
    "\n",
    "balanced_dfs = []\n",
    "for label in label_counts.index:\n",
    "    df_label = df_combined[df_combined['Label'] == label]\n",
    "    n_samples = min(len(df_label), target_per_class)\n",
    "    df_sampled = df_label.sample(n=n_samples, random_state=42)\n",
    "    balanced_dfs.append(df_sampled)\n",
    "    print(f\"  {label}: {n_samples:,} samples\")\n",
    "\n",
    "df_balanced = pd.concat(balanced_dfs, ignore_index=True)\n",
    "df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Free memory\n",
    "del df_combined\n",
    "gc.collect()\n",
    "\n",
    "print(f\"\\nBalanced dataset: {len(df_balanced):,} flows\")\n",
    "print(f\"Memory: {df_balanced.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Prepare Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X = df_balanced.drop(columns=['Label'])\n",
    "y = df_balanced['Label']\n",
    "\n",
    "# Handle missing/infinite values\n",
    "X = X.replace([np.inf, -np.inf], np.nan)\n",
    "X = X.fillna(0)\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Labels shape: {y.shape}\")\n",
    "print(f\"\\nClasses: {sorted(y.unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training: {len(X_train):,} samples\")\n",
    "print(f\"Testing: {len(X_test):,} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Train Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import time\n",
    "\n",
    "print(\"Training Random Forest (multiclass)...\")\n",
    "print(\"This may take 5-10 minutes.\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Optimized for memory and speed without losing accuracy\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=25,              # Limit depth for memory\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    max_features='sqrt',       # Memory efficient\n",
    "    class_weight='balanced',   # Handle imbalanced classes\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "rf_model.fit(X_train, y_train)\n",
    "training_time = time.time() - start\n",
    "\n",
    "print(f\"\\n✅ Training completed in {training_time:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "# Predict\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"MODEL PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nAccuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "classes = sorted(y.unique())\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=classes, yticklabels=classes)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model_path = MODELS_DIR / 'random_forest_nfstream_multiclass.joblib'\n",
    "joblib.dump(rf_model, model_path)\n",
    "print(f\"✅ Model saved: {model_path}\")\n",
    "\n",
    "# Save feature names\n",
    "features_path = MODELS_DIR / 'feature_names_nfstream_multiclass.joblib'\n",
    "joblib.dump(list(X.columns), features_path)\n",
    "print(f\"✅ Features saved: {features_path}\")\n",
    "\n",
    "# Save class names\n",
    "classes_path = MODELS_DIR / 'class_names_nfstream_multiclass.joblib'\n",
    "joblib.dump(sorted(y.unique()), classes_path)\n",
    "print(f\"✅ Classes saved: {classes_path}\")\n",
    "\n",
    "# Save info\n",
    "info_path = MODELS_DIR / 'nfstream_multiclass_info.txt'\n",
    "with open(info_path, 'w') as f:\n",
    "    f.write(f\"accuracy: {accuracy}\\n\")\n",
    "    f.write(f\"n_classes: {len(classes)}\\n\")\n",
    "    f.write(f\"n_features: {len(X.columns)}\\n\")\n",
    "    f.write(f\"classes: {sorted(y.unique())}\\n\")\n",
    "    f.write(f\"training_samples: {len(X_train)}\\n\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Model: {model_path.name}\")\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
    "print(f\"Classes: {sorted(y.unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Quick Test (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a small sample from Friday PCAP\n",
    "print(\"Testing model on fresh Friday PCAP sample...\")\n",
    "\n",
    "test_streamer = NFStreamer(\n",
    "    source=str(PCAP_FILES['Friday']),\n",
    "    statistical_analysis=True,\n",
    ")\n",
    "\n",
    "test_flows = []\n",
    "for i, flow in enumerate(test_streamer):\n",
    "    if i >= 10000:\n",
    "        break\n",
    "    flow_dict = {attr: np.float32(getattr(flow, attr, 0)) for attr in NFSTREAM_FEATURES}\n",
    "    test_flows.append(flow_dict)\n",
    "\n",
    "df_test = pd.DataFrame(test_flows)\n",
    "df_test = df_test.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "# Predict\n",
    "predictions = rf_model.predict(df_test)\n",
    "\n",
    "print(f\"\\nTest Results (10,000 flows):\")\n",
    "for label, count in pd.Series(predictions).value_counts().items():\n",
    "    pct = count / len(predictions) * 100\n",
    "    print(f\"  {label}: {count:,} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Next Steps\n",
    "\n",
    "1. Update `predictor.py` to load new model\n",
    "2. Update `analyzer.py` with `nfstream_multiclass` option\n",
    "3. Update `ai_service/main.py` to use new model\n",
    "4. Restart FastAPI and test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
