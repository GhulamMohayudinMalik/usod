{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass Network Threat Detection - CSV Training\n",
    "\n",
    "**Goal:** Train a multiclass model using the CICIDS2017 CSV dataset.\n",
    "\n",
    "## Attack Types Available:\n",
    "- BENIGN (normal traffic)\n",
    "- DDoS, PortScan\n",
    "- DoS Hulk, DoS GoldenEye, DoS slowloris, DoS Slowhttptest\n",
    "- FTP-Patator, SSH-Patator\n",
    "- Web Attack (Brute Force, XSS, SQL Injection)\n",
    "- Infiltration, Heartbleed\n",
    "\n",
    "## Strategy:\n",
    "1. Load all CSV files\n",
    "2. Clean and preprocess features\n",
    "3. Balance classes (undersample BENIGN, keep all attacks)\n",
    "4. Train Random Forest with optimized hyperparameters\n",
    "5. Evaluate and save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import time\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Paths\n",
    "BASE_DIR = Path('.').resolve().parent\n",
    "DATASET_DIR = BASE_DIR / 'dataset'\n",
    "MODELS_DIR = BASE_DIR / 'models'\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Base directory: {BASE_DIR}\")\n",
    "print(f\"Dataset directory: {DATASET_DIR}\")\n",
    "print(f\"Models directory: {MODELS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define Features (78 CICFlowMeter features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CICIDS2017 Feature Names (78 features)\n",
    "CICIDS2017_FEATURES = [\n",
    "    'Destination Port', 'Flow Duration', 'Total Fwd Packets', 'Total Backward Packets',\n",
    "    'Total Length of Fwd Packets', 'Total Length of Bwd Packets', 'Fwd Packet Length Max',\n",
    "    'Fwd Packet Length Min', 'Fwd Packet Length Mean', 'Fwd Packet Length Std',\n",
    "    'Bwd Packet Length Max', 'Bwd Packet Length Min', 'Bwd Packet Length Mean',\n",
    "    'Bwd Packet Length Std', 'Flow Bytes/s', 'Flow Packets/s', 'Flow IAT Mean',\n",
    "    'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Total', 'Fwd IAT Mean',\n",
    "    'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Total', 'Bwd IAT Mean',\n",
    "    'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags',\n",
    "    'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Length', 'Bwd Header Length',\n",
    "    'Fwd Packets/s', 'Bwd Packets/s', 'Min Packet Length', 'Max Packet Length',\n",
    "    'Packet Length Mean', 'Packet Length Std', 'Packet Length Variance',\n",
    "    'FIN Flag Count', 'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count',\n",
    "    'ACK Flag Count', 'URG Flag Count', 'CWE Flag Count', 'ECE Flag Count',\n",
    "    'Down/Up Ratio', 'Average Packet Size', 'Avg Fwd Segment Size', 'Avg Bwd Segment Size',\n",
    "    'Fwd Header Length.1', 'Fwd Avg Bytes/Bulk', 'Fwd Avg Packets/Bulk', 'Fwd Avg Bulk Rate',\n",
    "    'Bwd Avg Bytes/Bulk', 'Bwd Avg Packets/Bulk', 'Bwd Avg Bulk Rate', 'Subflow Fwd Packets',\n",
    "    'Subflow Fwd Bytes', 'Subflow Bwd Packets', 'Subflow Bwd Bytes', 'Init_Win_bytes_forward',\n",
    "    'Init_Win_bytes_backward', 'act_data_pkt_fwd', 'min_seg_size_forward', 'Active Mean',\n",
    "    'Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min'\n",
    "]\n",
    "\n",
    "print(f\"Total features: {len(CICIDS2017_FEATURES)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load All CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV files to load\n",
    "csv_files = [\n",
    "    'Monday-WorkingHours.pcap_ISCX.csv',\n",
    "    'Tuesday-WorkingHours.pcap_ISCX.csv',\n",
    "    'Wednesday-workingHours.pcap_ISCX.csv',\n",
    "    'Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv',\n",
    "    'Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv',\n",
    "    'Friday-WorkingHours-Morning.pcap_ISCX.csv',\n",
    "    'Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv',\n",
    "    'Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv',\n",
    "]\n",
    "\n",
    "# Check which files exist\n",
    "print(\"Checking CSV files:\")\n",
    "for f in csv_files:\n",
    "    path = DATASET_DIR / f\n",
    "    if path.exists():\n",
    "        size_mb = path.stat().st_size / (1024**2)\n",
    "        print(f\"  ✅ {f} ({size_mb:.1f} MB)\")\n",
    "    else:\n",
    "        print(f\"  ❌ {f} NOT FOUND\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_safe(filepath):\n",
    "    \"\"\"Load CSV with proper column handling.\"\"\"\n",
    "    df = pd.read_csv(filepath, low_memory=False)\n",
    "    df.columns = df.columns.str.strip()  # Remove whitespace from column names\n",
    "    return df\n",
    "\n",
    "# Load all CSV files\n",
    "print(\"Loading CSV files...\")\n",
    "dataframes = []\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    filepath = DATASET_DIR / csv_file\n",
    "    if filepath.exists():\n",
    "        print(f\"  Loading {csv_file}...\")\n",
    "        df = load_csv_safe(filepath)\n",
    "        print(f\"    Rows: {len(df):,}\")\n",
    "        print(f\"    Labels: {df['Label'].value_counts().to_dict()}\")\n",
    "        dataframes.append(df)\n",
    "    else:\n",
    "        print(f\"  ⚠️ Skipping {csv_file} (not found)\")\n",
    "\n",
    "print(f\"\\n✅ Loaded {len(dataframes)} CSV files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all dataframes\n",
    "print(\"Combining datasets...\")\n",
    "df_all = pd.concat(dataframes, ignore_index=True)\n",
    "print(f\"Total rows: {len(df_all):,}\")\n",
    "print(f\"Total columns: {len(df_all.columns)}\")\n",
    "\n",
    "# Show label distribution\n",
    "print(\"\\nLabel distribution:\")\n",
    "label_counts = df_all['Label'].value_counts()\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean labels (strip whitespace)\n",
    "df_all['Label'] = df_all['Label'].str.strip()\n",
    "\n",
    "# Group similar attack types for simplicity (optional)\n",
    "# You can comment this out if you want full granularity\n",
    "label_mapping = {\n",
    "    # Keep as-is\n",
    "    'BENIGN': 'BENIGN',\n",
    "    'DDoS': 'DDoS',\n",
    "    'PortScan': 'PortScan',\n",
    "    'FTP-Patator': 'Brute Force',\n",
    "    'SSH-Patator': 'Brute Force',\n",
    "    \n",
    "    # Group DoS variants\n",
    "    'DoS Hulk': 'DoS',\n",
    "    'DoS GoldenEye': 'DoS',\n",
    "    'DoS slowloris': 'DoS',\n",
    "    'DoS Slowhttptest': 'DoS',\n",
    "    'Heartbleed': 'DoS',\n",
    "    \n",
    "    # Group Web Attacks\n",
    "    'Web Attack � Brute Force': 'Web Attack',\n",
    "    'Web Attack � XSS': 'Web Attack',\n",
    "    'Web Attack � Sql Injection': 'Web Attack',\n",
    "    \n",
    "    # Keep Infiltration\n",
    "    'Infiltration': 'Infiltration',\n",
    "}\n",
    "\n",
    "# Apply mapping\n",
    "df_all['Label_Grouped'] = df_all['Label'].map(label_mapping)\n",
    "\n",
    "# Check for unmapped labels\n",
    "unmapped = df_all[df_all['Label_Grouped'].isna()]['Label'].unique()\n",
    "if len(unmapped) > 0:\n",
    "    print(f\"⚠️ Unmapped labels: {unmapped}\")\n",
    "    # Map any unmapped to 'Other'\n",
    "    df_all['Label_Grouped'] = df_all['Label_Grouped'].fillna('Other')\n",
    "\n",
    "print(\"\\nGrouped label distribution:\")\n",
    "print(df_all['Label_Grouped'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the features we need\n",
    "available_features = [f for f in CICIDS2017_FEATURES if f in df_all.columns]\n",
    "missing_features = [f for f in CICIDS2017_FEATURES if f not in df_all.columns]\n",
    "\n",
    "print(f\"Available features: {len(available_features)}/{len(CICIDS2017_FEATURES)}\")\n",
    "if missing_features:\n",
    "    print(f\"Missing features: {missing_features}\")\n",
    "\n",
    "# Create feature matrix\n",
    "X = df_all[available_features].copy()\n",
    "y = df_all['Label_Grouped'].copy()  # Use grouped labels\n",
    "\n",
    "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "print(f\"Label vector shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing and infinite values\n",
    "print(\"Cleaning data...\")\n",
    "print(f\"  Missing values before: {X.isnull().sum().sum():,}\")\n",
    "print(f\"  Infinite values before: {np.isinf(X.select_dtypes(include=[np.number])).sum().sum():,}\")\n",
    "\n",
    "# Replace infinities with NaN, then fill NaN\n",
    "X = X.replace([np.inf, -np.inf], np.nan)\n",
    "X = X.fillna(0)\n",
    "\n",
    "print(f\"  Missing values after: {X.isnull().sum().sum():.0f}\")\n",
    "print(f\"  Infinite values after: {np.isinf(X.select_dtypes(include=[np.number])).sum().sum():.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Balance Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Current class distribution\n",
    "print(\"Current class distribution:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# Balance strategy:\n",
    "# - BENIGN is heavily overrepresented, undersample to max 200K\n",
    "# - Keep all attack samples (they are minority)\n",
    "# - Oversample very rare attacks to minimum 1000 samples\n",
    "\n",
    "MAX_BENIGN = 200000\n",
    "MIN_ATTACK = 1000\n",
    "\n",
    "balanced_dfs = []\n",
    "\n",
    "for label in y.unique():\n",
    "    df_class = pd.concat([X[y == label], y[y == label]], axis=1)\n",
    "    n_samples = len(df_class)\n",
    "    \n",
    "    if label == 'BENIGN':\n",
    "        if n_samples > MAX_BENIGN:\n",
    "            df_class = resample(df_class, n_samples=MAX_BENIGN, random_state=42, replace=False)\n",
    "            print(f\"  {label}: {n_samples:,} -> {MAX_BENIGN:,} (undersampled)\")\n",
    "        else:\n",
    "            print(f\"  {label}: {n_samples:,} (kept)\")\n",
    "    else:\n",
    "        if n_samples < MIN_ATTACK:\n",
    "            df_class = resample(df_class, n_samples=MIN_ATTACK, random_state=42, replace=True)\n",
    "            print(f\"  {label}: {n_samples:,} -> {MIN_ATTACK:,} (oversampled)\")\n",
    "        else:\n",
    "            print(f\"  {label}: {n_samples:,} (kept)\")\n",
    "    \n",
    "    balanced_dfs.append(df_class)\n",
    "\n",
    "# Combine balanced data\n",
    "df_balanced = pd.concat(balanced_dfs, ignore_index=True)\n",
    "df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)  # Shuffle\n",
    "\n",
    "X_balanced = df_balanced.drop(columns=['Label_Grouped'])\n",
    "y_balanced = df_balanced['Label_Grouped']\n",
    "\n",
    "print(f\"\\nBalanced dataset: {len(df_balanced):,} samples\")\n",
    "print(y_balanced.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_balanced, y_balanced,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_balanced\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train):,} samples\")\n",
    "print(f\"Test set: {len(X_test):,} samples\")\n",
    "\n",
    "print(\"\\nTraining label distribution:\")\n",
    "print(y_train.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Train Random Forest (Optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "print(\"Training Random Forest Classifier...\")\n",
    "print(\"=\"*60)\n",
    "print(\"Using optimized hyperparameters for smaller model size\")\n",
    "\n",
    "# Optimized hyperparameters for smaller model\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=50,        # Reduced from 100 for smaller model\n",
    "    max_depth=20,           # Limit depth to prevent overfitting and reduce size\n",
    "    min_samples_split=10,   # Require more samples to split\n",
    "    min_samples_leaf=5,     # Require more samples in leaf nodes\n",
    "    max_features='sqrt',    # Use sqrt of features for each tree\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    class_weight='balanced',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "rf_model.fit(X_train, y_train)\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n✅ Training completed in {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL PERFORMANCE (Test Set)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "class_names = sorted(y_test.unique())\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Confusion Matrix - Multiclass Model')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save model\n",
    "model_path = MODELS_DIR / 'random_forest_multiclass_cicids.joblib'\n",
    "joblib.dump(rf_model, model_path)\n",
    "\n",
    "# Save feature names\n",
    "feature_names_path = MODELS_DIR / 'feature_names_multiclass_cicids.joblib'\n",
    "joblib.dump(list(X_train.columns), feature_names_path)\n",
    "\n",
    "# Save class names\n",
    "class_names_path = MODELS_DIR / 'class_names_multiclass_cicids.joblib'\n",
    "joblib.dump(sorted(y_train.unique().tolist()), class_names_path)\n",
    "\n",
    "# Check model size\n",
    "model_size_mb = model_path.stat().st_size / (1024**2)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL SAVED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model: {model_path}\")\n",
    "print(f\"Size: {model_size_mb:.2f} MB\")\n",
    "print(f\"Features: {len(X_train.columns)}\")\n",
    "print(f\"Classes: {sorted(y_train.unique().tolist())}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 20 Most Important Features:\")\n",
    "print(feature_importance.head(20).to_string(index=False))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(feature_importance.head(20)['feature'][::-1], \n",
    "         feature_importance.head(20)['importance'][::-1])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 20 Feature Importances')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Model trained successfully!**\n",
    "\n",
    "### Next Steps:\n",
    "1. Update `src/predictor.py` to load this new model\n",
    "2. Update `src/feature_extractor.py` to use CICFlowMeter plugin for PCAP extraction\n",
    "3. Test on real PCAP files\n",
    "\n",
    "### Model Files:\n",
    "- `models/random_forest_multiclass_cicids.joblib`\n",
    "- `models/feature_names_multiclass_cicids.joblib`\n",
    "- `models/class_names_multiclass_cicids.joblib`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
