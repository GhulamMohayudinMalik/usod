{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NFStream-Optimized Model Training\n",
        "\n",
        "This notebook trains a model specifically optimized for NFStream feature extraction.\n",
        "\n",
        "## Why This Notebook?\n",
        "- Original model trained on CICFlowMeter features\n",
        "- NFStream extracts features with different names/calculations\n",
        "- This model is trained on NFStream-compatible features for better PCAP accuracy\n",
        "\n",
        "## Approach\n",
        "1. Load CICIDS2017 CSV data (with labels)\n",
        "2. Map CICFlowMeter features → NFStream feature names\n",
        "3. Train model on NFStream-named features\n",
        "4. Test with actual PCAP extraction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports and Setup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set paths\n",
        "BASE_DIR = Path.cwd().parent\n",
        "DATASET_DIR = BASE_DIR / 'dataset'\n",
        "MODELS_DIR = BASE_DIR / 'models'\n",
        "RESULTS_DIR = BASE_DIR / 'results'\n",
        "PCAP_DIR = BASE_DIR / 'pcap'\n",
        "\n",
        "print(f\"Base directory: {BASE_DIR}\")\n",
        "print(f\"Dataset directory: {DATASET_DIR}\")\n",
        "print(f\"Models directory: {MODELS_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Define Feature Mapping\n",
        "\n",
        "Map CICFlowMeter (CICIDS2017) feature names to NFStream feature names.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CICFlowMeter to NFStream Feature Mapping\n",
        "# This maps CICIDS2017 CSV column names to NFStream attribute names\n",
        "\n",
        "CICIDS_TO_NFSTREAM = {\n",
        "    # Port and protocol\n",
        "    'Destination Port': 'dst_port',\n",
        "    \n",
        "    # Duration\n",
        "    'Flow Duration': 'bidirectional_duration_ms',\n",
        "    \n",
        "    # Packet counts\n",
        "    'Total Fwd Packets': 'src2dst_packets',\n",
        "    'Total Backward Packets': 'dst2src_packets',\n",
        "    \n",
        "    # Byte counts\n",
        "    'Total Length of Fwd Packets': 'src2dst_bytes',\n",
        "    'Total Length of Bwd Packets': 'dst2src_bytes',\n",
        "    \n",
        "    # Forward packet length stats\n",
        "    'Fwd Packet Length Max': 'src2dst_max_ps',\n",
        "    'Fwd Packet Length Min': 'src2dst_min_ps',\n",
        "    'Fwd Packet Length Mean': 'src2dst_mean_ps',\n",
        "    'Fwd Packet Length Std': 'src2dst_stddev_ps',\n",
        "    \n",
        "    # Backward packet length stats\n",
        "    'Bwd Packet Length Max': 'dst2src_max_ps',\n",
        "    'Bwd Packet Length Min': 'dst2src_min_ps',\n",
        "    'Bwd Packet Length Mean': 'dst2src_mean_ps',\n",
        "    'Bwd Packet Length Std': 'dst2src_stddev_ps',\n",
        "    \n",
        "    # Flow-level stats (bidirectional)\n",
        "    'Min Packet Length': 'bidirectional_min_ps',\n",
        "    'Max Packet Length': 'bidirectional_max_ps',\n",
        "    'Packet Length Mean': 'bidirectional_mean_ps',\n",
        "    'Packet Length Std': 'bidirectional_stddev_ps',\n",
        "    \n",
        "    # Inter-arrival times (flow level)\n",
        "    'Flow IAT Mean': 'bidirectional_mean_piat_ms',\n",
        "    'Flow IAT Std': 'bidirectional_stddev_piat_ms',\n",
        "    'Flow IAT Max': 'bidirectional_max_piat_ms',\n",
        "    'Flow IAT Min': 'bidirectional_min_piat_ms',\n",
        "    \n",
        "    # Forward IAT\n",
        "    'Fwd IAT Total': 'src2dst_duration_ms',\n",
        "    'Fwd IAT Mean': 'src2dst_mean_piat_ms',\n",
        "    'Fwd IAT Std': 'src2dst_stddev_piat_ms',\n",
        "    'Fwd IAT Max': 'src2dst_max_piat_ms',\n",
        "    'Fwd IAT Min': 'src2dst_min_piat_ms',\n",
        "    \n",
        "    # Backward IAT\n",
        "    'Bwd IAT Total': 'dst2src_duration_ms',\n",
        "    'Bwd IAT Mean': 'dst2src_mean_piat_ms',\n",
        "    'Bwd IAT Std': 'dst2src_stddev_piat_ms',\n",
        "    'Bwd IAT Max': 'dst2src_max_piat_ms',\n",
        "    'Bwd IAT Min': 'dst2src_min_piat_ms',\n",
        "    \n",
        "    # TCP Flags - Forward\n",
        "    'Fwd PSH Flags': 'src2dst_psh_packets',\n",
        "    'Fwd URG Flags': 'src2dst_urg_packets',\n",
        "    \n",
        "    # TCP Flags - Backward\n",
        "    'Bwd PSH Flags': 'dst2src_psh_packets',\n",
        "    'Bwd URG Flags': 'dst2src_urg_packets',\n",
        "    \n",
        "    # TCP Flags - Combined (we'll compute from src2dst + dst2src)\n",
        "    'FIN Flag Count': 'fin_flag_count',  # Derived\n",
        "    'SYN Flag Count': 'syn_flag_count',  # Derived\n",
        "    'RST Flag Count': 'rst_flag_count',  # Derived\n",
        "    'PSH Flag Count': 'psh_flag_count',  # Derived\n",
        "    'ACK Flag Count': 'ack_flag_count',  # Derived\n",
        "    'URG Flag Count': 'urg_flag_count',  # Derived\n",
        "}\n",
        "\n",
        "# Features that NFStream can provide\n",
        "NFSTREAM_AVAILABLE_FEATURES = [\n",
        "    'dst_port',\n",
        "    'bidirectional_duration_ms',\n",
        "    'src2dst_packets', 'dst2src_packets',\n",
        "    'src2dst_bytes', 'dst2src_bytes',\n",
        "    'src2dst_max_ps', 'src2dst_min_ps', 'src2dst_mean_ps', 'src2dst_stddev_ps',\n",
        "    'dst2src_max_ps', 'dst2src_min_ps', 'dst2src_mean_ps', 'dst2src_stddev_ps',\n",
        "    'bidirectional_min_ps', 'bidirectional_max_ps', 'bidirectional_mean_ps', 'bidirectional_stddev_ps',\n",
        "    'bidirectional_mean_piat_ms', 'bidirectional_stddev_piat_ms', 'bidirectional_max_piat_ms', 'bidirectional_min_piat_ms',\n",
        "    'src2dst_duration_ms', 'src2dst_mean_piat_ms', 'src2dst_stddev_piat_ms', 'src2dst_max_piat_ms', 'src2dst_min_piat_ms',\n",
        "    'dst2src_duration_ms', 'dst2src_mean_piat_ms', 'dst2src_stddev_piat_ms', 'dst2src_max_piat_ms', 'dst2src_min_piat_ms',\n",
        "    'src2dst_psh_packets', 'src2dst_urg_packets', 'src2dst_syn_packets', 'src2dst_fin_packets', 'src2dst_rst_packets', 'src2dst_ack_packets',\n",
        "    'dst2src_psh_packets', 'dst2src_urg_packets', 'dst2src_syn_packets', 'dst2src_fin_packets', 'dst2src_rst_packets', 'dst2src_ack_packets',\n",
        "    # Derived features we'll calculate\n",
        "    'bidirectional_packets', 'bidirectional_bytes',\n",
        "    'flow_bytes_per_second', 'flow_packets_per_second',\n",
        "    'fwd_packets_per_second', 'bwd_packets_per_second',\n",
        "    'packet_length_variance', 'down_up_ratio', 'average_packet_size',\n",
        "]\n",
        "\n",
        "print(f\"CICFlowMeter to NFStream mappings defined: {len(CICIDS_TO_NFSTREAM)}\")\n",
        "print(f\"NFStream available features: {len(NFSTREAM_AVAILABLE_FEATURES)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load CICIDS2017 Data and Transform to NFStream Features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load all CICIDS2017 CSV files\n",
        "csv_files = sorted(DATASET_DIR.glob('*.csv'))\n",
        "print(f\"Found {len(csv_files)} CSV files\")\n",
        "\n",
        "dataframes = []\n",
        "for file in csv_files:\n",
        "    print(f\"Loading {file.name}...\")\n",
        "    df_temp = pd.read_csv(file, low_memory=False)\n",
        "    df_temp.columns = df_temp.columns.str.strip()\n",
        "    print(f\"  Shape: {df_temp.shape}\")\n",
        "    dataframes.append(df_temp)\n",
        "\n",
        "# Combine all dataframes\n",
        "print(\"\\nCombining all dataframes...\")\n",
        "df = pd.concat(dataframes, ignore_index=True)\n",
        "print(f\"Combined dataset shape: {df.shape}\")\n",
        "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Transform CICIDS2017 data to NFStream-compatible features\n",
        "def transform_to_nfstream_features(df_cicids):\n",
        "    \"\"\"\n",
        "    Transform CICIDS2017 dataframe to NFStream-compatible feature names and format.\n",
        "    \"\"\"\n",
        "    df_nf = pd.DataFrame()\n",
        "    \n",
        "    # Direct mappings\n",
        "    df_nf['dst_port'] = df_cicids.get('Destination Port', 0)\n",
        "    df_nf['bidirectional_duration_ms'] = df_cicids.get('Flow Duration', 0)\n",
        "    \n",
        "    # Packet counts\n",
        "    df_nf['src2dst_packets'] = df_cicids.get('Total Fwd Packets', 0)\n",
        "    df_nf['dst2src_packets'] = df_cicids.get('Total Backward Packets', 0)\n",
        "    df_nf['bidirectional_packets'] = df_nf['src2dst_packets'] + df_nf['dst2src_packets']\n",
        "    \n",
        "    # Byte counts\n",
        "    df_nf['src2dst_bytes'] = df_cicids.get('Total Length of Fwd Packets', 0)\n",
        "    df_nf['dst2src_bytes'] = df_cicids.get('Total Length of Bwd Packets', 0)\n",
        "    df_nf['bidirectional_bytes'] = df_nf['src2dst_bytes'] + df_nf['dst2src_bytes']\n",
        "    \n",
        "    # Forward packet length stats\n",
        "    df_nf['src2dst_max_ps'] = df_cicids.get('Fwd Packet Length Max', 0)\n",
        "    df_nf['src2dst_min_ps'] = df_cicids.get('Fwd Packet Length Min', 0)\n",
        "    df_nf['src2dst_mean_ps'] = df_cicids.get('Fwd Packet Length Mean', 0)\n",
        "    df_nf['src2dst_stddev_ps'] = df_cicids.get('Fwd Packet Length Std', 0)\n",
        "    \n",
        "    # Backward packet length stats\n",
        "    df_nf['dst2src_max_ps'] = df_cicids.get('Bwd Packet Length Max', 0)\n",
        "    df_nf['dst2src_min_ps'] = df_cicids.get('Bwd Packet Length Min', 0)\n",
        "    df_nf['dst2src_mean_ps'] = df_cicids.get('Bwd Packet Length Mean', 0)\n",
        "    df_nf['dst2src_stddev_ps'] = df_cicids.get('Bwd Packet Length Std', 0)\n",
        "    \n",
        "    # Bidirectional packet length stats\n",
        "    df_nf['bidirectional_min_ps'] = df_cicids.get('Min Packet Length', 0)\n",
        "    df_nf['bidirectional_max_ps'] = df_cicids.get('Max Packet Length', 0)\n",
        "    df_nf['bidirectional_mean_ps'] = df_cicids.get('Packet Length Mean', 0)\n",
        "    df_nf['bidirectional_stddev_ps'] = df_cicids.get('Packet Length Std', 0)\n",
        "    \n",
        "    # Flow IAT\n",
        "    df_nf['bidirectional_mean_piat_ms'] = df_cicids.get('Flow IAT Mean', 0)\n",
        "    df_nf['bidirectional_stddev_piat_ms'] = df_cicids.get('Flow IAT Std', 0)\n",
        "    df_nf['bidirectional_max_piat_ms'] = df_cicids.get('Flow IAT Max', 0)\n",
        "    df_nf['bidirectional_min_piat_ms'] = df_cicids.get('Flow IAT Min', 0)\n",
        "    \n",
        "    # Forward IAT\n",
        "    df_nf['src2dst_duration_ms'] = df_cicids.get('Fwd IAT Total', 0)\n",
        "    df_nf['src2dst_mean_piat_ms'] = df_cicids.get('Fwd IAT Mean', 0)\n",
        "    df_nf['src2dst_stddev_piat_ms'] = df_cicids.get('Fwd IAT Std', 0)\n",
        "    df_nf['src2dst_max_piat_ms'] = df_cicids.get('Fwd IAT Max', 0)\n",
        "    df_nf['src2dst_min_piat_ms'] = df_cicids.get('Fwd IAT Min', 0)\n",
        "    \n",
        "    # Backward IAT\n",
        "    df_nf['dst2src_duration_ms'] = df_cicids.get('Bwd IAT Total', 0)\n",
        "    df_nf['dst2src_mean_piat_ms'] = df_cicids.get('Bwd IAT Mean', 0)\n",
        "    df_nf['dst2src_stddev_piat_ms'] = df_cicids.get('Bwd IAT Std', 0)\n",
        "    df_nf['dst2src_max_piat_ms'] = df_cicids.get('Bwd IAT Max', 0)\n",
        "    df_nf['dst2src_min_piat_ms'] = df_cicids.get('Bwd IAT Min', 0)\n",
        "    \n",
        "    # TCP Flags\n",
        "    df_nf['src2dst_psh_packets'] = df_cicids.get('Fwd PSH Flags', 0)\n",
        "    df_nf['src2dst_urg_packets'] = df_cicids.get('Fwd URG Flags', 0)\n",
        "    df_nf['dst2src_psh_packets'] = df_cicids.get('Bwd PSH Flags', 0)\n",
        "    df_nf['dst2src_urg_packets'] = df_cicids.get('Bwd URG Flags', 0)\n",
        "    \n",
        "    # Combined flags\n",
        "    df_nf['src2dst_syn_packets'] = df_cicids.get('SYN Flag Count', 0)\n",
        "    df_nf['src2dst_fin_packets'] = df_cicids.get('FIN Flag Count', 0)\n",
        "    df_nf['src2dst_rst_packets'] = df_cicids.get('RST Flag Count', 0)\n",
        "    df_nf['src2dst_ack_packets'] = df_cicids.get('ACK Flag Count', 0)\n",
        "    df_nf['dst2src_syn_packets'] = 0  # Not available separately\n",
        "    df_nf['dst2src_fin_packets'] = 0\n",
        "    df_nf['dst2src_rst_packets'] = 0\n",
        "    df_nf['dst2src_ack_packets'] = 0\n",
        "    \n",
        "    # Derived features (calculated same way as NFStream)\n",
        "    duration_s = (df_nf['bidirectional_duration_ms'] / 1000).replace(0, 0.001)\n",
        "    df_nf['flow_bytes_per_second'] = df_nf['bidirectional_bytes'] / duration_s\n",
        "    df_nf['flow_packets_per_second'] = df_nf['bidirectional_packets'] / duration_s\n",
        "    df_nf['fwd_packets_per_second'] = df_nf['src2dst_packets'] / duration_s\n",
        "    df_nf['bwd_packets_per_second'] = df_nf['dst2src_packets'] / duration_s\n",
        "    \n",
        "    # More derived features\n",
        "    df_nf['packet_length_variance'] = df_nf['bidirectional_stddev_ps'] ** 2\n",
        "    df_nf['down_up_ratio'] = df_nf['dst2src_packets'] / df_nf['src2dst_packets'].replace(0, 1)\n",
        "    df_nf['average_packet_size'] = df_nf['bidirectional_bytes'] / df_nf['bidirectional_packets'].replace(0, 1)\n",
        "    \n",
        "    return df_nf\n",
        "\n",
        "# Transform the data\n",
        "print(\"Transforming CICIDS2017 data to NFStream features...\")\n",
        "df_nfstream = transform_to_nfstream_features(df)\n",
        "print(f\"Transformed shape: {df_nfstream.shape}\")\n",
        "print(f\"\\nNFStream feature columns ({len(df_nfstream.columns)}):\")\n",
        "print(list(df_nfstream.columns))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Prepare Labels and Preprocess Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get labels from original data\n",
        "label_col = 'Label'\n",
        "y_original = df[label_col].copy()\n",
        "\n",
        "print(f\"Label distribution:\")\n",
        "print(y_original.value_counts())\n",
        "\n",
        "# Handle class imbalance: combine rare classes into 'Other'\n",
        "label_counts = y_original.value_counts()\n",
        "rare_threshold = 100\n",
        "rare_classes = label_counts[label_counts < rare_threshold].index.tolist()\n",
        "\n",
        "print(f\"\\nRare classes (< {rare_threshold} samples): {rare_classes}\")\n",
        "\n",
        "# Create multiclass labels\n",
        "y = y_original.copy()\n",
        "if rare_classes:\n",
        "    y = y.replace(rare_classes, 'Other')\n",
        "    print(f\"\\nAfter combining rare classes:\")\n",
        "    print(y.value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocess: Handle infinite and missing values\n",
        "X = df_nfstream.copy()\n",
        "\n",
        "print(f\"Features shape: {X.shape}\")\n",
        "\n",
        "# Handle infinite values\n",
        "print(\"Handling infinite values...\")\n",
        "X = X.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "# Fill missing values with median\n",
        "print(\"Filling missing values...\")\n",
        "for col in X.columns:\n",
        "    if X[col].isnull().sum() > 0:\n",
        "        X[col] = X[col].fillna(X[col].median())\n",
        "\n",
        "# Final check\n",
        "print(f\"Missing values: {X.isnull().sum().sum()}\")\n",
        "print(f\"Infinite values: {np.isinf(X.select_dtypes(include=[np.number])).sum().sum()}\")\n",
        "print(f\"\\nFinal features shape: {X.shape}\")\n",
        "print(f\"Final labels shape: {y.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Train-Test Split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, \n",
        "    test_size=0.2, \n",
        "    random_state=42, \n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set shape: {X_train.shape}\")\n",
        "print(f\"Test set shape: {X_test.shape}\")\n",
        "print(f\"\\nTraining set class distribution:\")\n",
        "print(y_train.value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Train NFStream-Optimized Random Forest Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import time\n",
        "\n",
        "print(\"Training NFStream-Optimized Random Forest Classifier...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Initialize Random Forest with class_weight='balanced'\n",
        "rf_nfstream = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    class_weight='balanced',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "start_time = time.time()\n",
        "rf_nfstream.fit(X_train, y_train)\n",
        "training_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\n✅ Training completed in {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Evaluate Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    classification_report, confusion_matrix\n",
        ")\n",
        "\n",
        "# Make predictions\n",
        "print(\"Making predictions on test set...\")\n",
        "y_pred = rf_nfstream.predict(X_test)\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision_macro = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
        "recall_macro = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
        "f1_macro = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"NFSTREAM-OPTIMIZED MODEL PERFORMANCE\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Accuracy:        {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "print(f\"Precision (Macro): {precision_macro:.4f}\")\n",
        "print(f\"Recall (Macro):    {recall_macro:.4f}\")\n",
        "print(f\"F1-Score (Macro):  {f1_macro:.4f}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Detailed report\n",
        "print(\"\\nDetailed Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, zero_division=0))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Save NFStream-Optimized Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import joblib\n",
        "from datetime import datetime\n",
        "\n",
        "# Create models directory\n",
        "MODELS_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# Save model\n",
        "model_filename = MODELS_DIR / 'random_forest_nfstream_optimized.joblib'\n",
        "joblib.dump(rf_nfstream, model_filename)\n",
        "print(f\"✅ Model saved to: {model_filename}\")\n",
        "\n",
        "# Save feature names (NFStream format)\n",
        "feature_names_file = MODELS_DIR / 'feature_names_nfstream.joblib'\n",
        "joblib.dump(list(X.columns), feature_names_file)\n",
        "print(f\"✅ Feature names saved to: {feature_names_file}\")\n",
        "\n",
        "# Save class names\n",
        "class_names = sorted(y.unique())\n",
        "class_names_file = MODELS_DIR / 'class_names_nfstream.joblib'\n",
        "joblib.dump(class_names, class_names_file)\n",
        "print(f\"✅ Class names saved to: {class_names_file}\")\n",
        "\n",
        "print(f\"\\nFeatures ({len(X.columns)}): {list(X.columns)[:10]}...\")\n",
        "print(f\"Classes ({len(class_names)}): {class_names}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Test with Real PCAP File\n",
        "\n",
        "Now let's test the NFStream-optimized model with actual PCAP extraction to verify it works correctly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test with Monday PCAP file\n",
        "from nfstream import NFStreamer\n",
        "\n",
        "pcap_file = PCAP_DIR / 'Monday-WorkingHours.pcap'\n",
        "print(f\"PCAP file: {pcap_file}\")\n",
        "print(f\"File exists: {pcap_file.exists()}\")\n",
        "\n",
        "if pcap_file.exists():\n",
        "    print(f\"\\nExtracting features from PCAP using NFStream...\")\n",
        "    \n",
        "    # Define attributes to extract\n",
        "    FLOW_ATTRIBUTES = [\n",
        "        'dst_port', 'bidirectional_duration_ms',\n",
        "        'src2dst_packets', 'dst2src_packets', 'bidirectional_packets',\n",
        "        'src2dst_bytes', 'dst2src_bytes', 'bidirectional_bytes',\n",
        "        'src2dst_max_ps', 'src2dst_min_ps', 'src2dst_mean_ps', 'src2dst_stddev_ps',\n",
        "        'dst2src_max_ps', 'dst2src_min_ps', 'dst2src_mean_ps', 'dst2src_stddev_ps',\n",
        "        'bidirectional_min_ps', 'bidirectional_max_ps', 'bidirectional_mean_ps', 'bidirectional_stddev_ps',\n",
        "        'bidirectional_mean_piat_ms', 'bidirectional_stddev_piat_ms', 'bidirectional_max_piat_ms', 'bidirectional_min_piat_ms',\n",
        "        'src2dst_duration_ms', 'src2dst_mean_piat_ms', 'src2dst_stddev_piat_ms', 'src2dst_max_piat_ms', 'src2dst_min_piat_ms',\n",
        "        'dst2src_duration_ms', 'dst2src_mean_piat_ms', 'dst2src_stddev_piat_ms', 'dst2src_max_piat_ms', 'dst2src_min_piat_ms',\n",
        "        'src2dst_psh_packets', 'src2dst_urg_packets', 'src2dst_syn_packets', 'src2dst_fin_packets', 'src2dst_rst_packets', 'src2dst_ack_packets',\n",
        "        'dst2src_psh_packets', 'dst2src_urg_packets', 'dst2src_syn_packets', 'dst2src_fin_packets', 'dst2src_rst_packets', 'dst2src_ack_packets',\n",
        "    ]\n",
        "    \n",
        "    # Extract flows\n",
        "    streamer = NFStreamer(\n",
        "        source=str(pcap_file),\n",
        "        statistical_analysis=True,\n",
        "        splt_analysis=0,\n",
        "        n_dissections=0,\n",
        "    )\n",
        "    \n",
        "    MAX_FLOWS = 50000  # Limit for quick testing\n",
        "    flows_list = []\n",
        "    \n",
        "    print(f\"Extracting up to {MAX_FLOWS:,} flows...\")\n",
        "    for i, flow in enumerate(streamer):\n",
        "        flow_dict = {}\n",
        "        for attr in FLOW_ATTRIBUTES:\n",
        "            try:\n",
        "                flow_dict[attr] = getattr(flow, attr, 0)\n",
        "            except:\n",
        "                flow_dict[attr] = 0\n",
        "        flows_list.append(flow_dict)\n",
        "        \n",
        "        if (i + 1) % 10000 == 0:\n",
        "            print(f\"  Processed {i+1:,} flows...\")\n",
        "        \n",
        "        if i + 1 >= MAX_FLOWS:\n",
        "            break\n",
        "    \n",
        "    print(f\"\\n✅ Extracted {len(flows_list):,} flows\")\n",
        "    \n",
        "    # Convert to DataFrame\n",
        "    df_pcap = pd.DataFrame(flows_list)\n",
        "    print(f\"PCAP DataFrame shape: {df_pcap.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add derived features (same as during training)\n",
        "if 'df_pcap' in dir():\n",
        "    print(\"Adding derived features...\")\n",
        "    \n",
        "    # Calculate derived features\n",
        "    duration_s = (df_pcap['bidirectional_duration_ms'] / 1000).replace(0, 0.001)\n",
        "    df_pcap['flow_bytes_per_second'] = df_pcap['bidirectional_bytes'] / duration_s\n",
        "    df_pcap['flow_packets_per_second'] = df_pcap['bidirectional_packets'] / duration_s\n",
        "    df_pcap['fwd_packets_per_second'] = df_pcap['src2dst_packets'] / duration_s\n",
        "    df_pcap['bwd_packets_per_second'] = df_pcap['dst2src_packets'] / duration_s\n",
        "    df_pcap['packet_length_variance'] = df_pcap['bidirectional_stddev_ps'] ** 2\n",
        "    df_pcap['down_up_ratio'] = df_pcap['dst2src_packets'] / df_pcap['src2dst_packets'].replace(0, 1)\n",
        "    df_pcap['average_packet_size'] = df_pcap['bidirectional_bytes'] / df_pcap['bidirectional_packets'].replace(0, 1)\n",
        "    \n",
        "    # Ensure all required features exist\n",
        "    required_features = list(X.columns)\n",
        "    for feat in required_features:\n",
        "        if feat not in df_pcap.columns:\n",
        "            df_pcap[feat] = 0\n",
        "    \n",
        "    # Select features in correct order\n",
        "    X_pcap = df_pcap[required_features].copy()\n",
        "    \n",
        "    # Handle infinite/missing values\n",
        "    X_pcap = X_pcap.replace([np.inf, -np.inf], np.nan)\n",
        "    X_pcap = X_pcap.fillna(0)\n",
        "    \n",
        "    print(f\"PCAP features shape: {X_pcap.shape}\")\n",
        "    print(f\"Features match training: {list(X_pcap.columns) == required_features}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make predictions on PCAP data\n",
        "if 'X_pcap' in dir():\n",
        "    print(\"Making predictions on PCAP data...\")\n",
        "    pcap_predictions = rf_nfstream.predict(X_pcap)\n",
        "    \n",
        "    # Show results\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"PCAP ANALYSIS RESULTS (NFStream-Optimized Model)\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Total flows analyzed: {len(pcap_predictions):,}\")\n",
        "    \n",
        "    # Prediction distribution\n",
        "    pred_counts = pd.Series(pcap_predictions).value_counts()\n",
        "    print(f\"\\nPrediction Distribution:\")\n",
        "    for label, count in pred_counts.items():\n",
        "        pct = count / len(pcap_predictions) * 100\n",
        "        print(f\"  {label}: {count:,} ({pct:.2f}%)\")\n",
        "    \n",
        "    # Check if all predictions are BENIGN (expected for Monday)\n",
        "    benign_count = (pcap_predictions == 'BENIGN').sum()\n",
        "    print(f\"\\n{'✅' if benign_count == len(pcap_predictions) else '⚠️'} BENIGN traffic: {benign_count:,} ({benign_count/len(pcap_predictions)*100:.2f}%)\")\n",
        "    \n",
        "    if benign_count == len(pcap_predictions):\n",
        "        print(\"\\n✅ SUCCESS! All Monday traffic correctly classified as BENIGN!\")\n",
        "        print(\"   The NFStream-optimized model is working correctly.\")\n",
        "    else:\n",
        "        print(f\"\\n⚠️ Some flows classified as attacks: {len(pcap_predictions) - benign_count:,}\")\n",
        "        print(\"   (This could be false positives due to feature differences)\")\n",
        "    \n",
        "    print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### What This Notebook Created:\n",
        "\n",
        "1. **NFStream-Optimized Model** (`random_forest_nfstream_optimized.joblib`)\n",
        "   - Trained on NFStream-compatible feature names\n",
        "   - Ready for production PCAP analysis\n",
        "\n",
        "2. **Feature Names** (`feature_names_nfstream.joblib`)\n",
        "   - List of features in NFStream format\n",
        "   - Used by the feature extractor\n",
        "\n",
        "3. **Class Names** (`class_names_nfstream.joblib`)\n",
        "   - Attack type labels\n",
        "\n",
        "### Usage:\n",
        "```python\n",
        "# Load model\n",
        "import joblib\n",
        "model = joblib.load('models/random_forest_nfstream_optimized.joblib')\n",
        "features = joblib.load('models/feature_names_nfstream.joblib')\n",
        "\n",
        "# Extract features from PCAP with NFStream\n",
        "# Ensure feature order matches training\n",
        "# Make predictions\n",
        "predictions = model.predict(X_pcap[features])\n",
        "```\n",
        "\n",
        "### Next Steps:\n",
        "1. Update `src/predictor.py` to use the new NFStream-optimized model\n",
        "2. Test with PCAP files containing attacks\n",
        "3. Integrate into web application\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
