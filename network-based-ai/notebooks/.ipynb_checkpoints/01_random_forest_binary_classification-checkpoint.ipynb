{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Network Threat Detection - Random Forest Binary Classification\n",
        "\n",
        "This notebook trains a Random Forest model to classify network traffic as BENIGN or ATTACK.\n",
        "\n",
        "**Approach:**\n",
        "- Load CICIDS2017 dataset\n",
        "- Preprocess data (handle missing values, convert to binary classification)\n",
        "- Train Random Forest model\n",
        "- Evaluate performance\n",
        "- Save model for later use\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set paths\n",
        "BASE_DIR = Path.cwd().parent\n",
        "DATASET_DIR = BASE_DIR / 'dataset'\n",
        "MODELS_DIR = BASE_DIR / 'models'\n",
        "RESULTS_DIR = BASE_DIR / 'results'\n",
        "\n",
        "print(f\"Base directory: {BASE_DIR}\")\n",
        "print(f\"Dataset directory: {DATASET_DIR}\")\n",
        "print(f\"Dataset files: {list(DATASET_DIR.glob('*.csv'))}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get all CSV files\n",
        "csv_files = sorted(DATASET_DIR.glob('*.csv'))\n",
        "print(f\"Found {len(csv_files)} CSV files\")\n",
        "\n",
        "# Load all CSV files\n",
        "dataframes = []\n",
        "for file in csv_files:\n",
        "    print(f\"Loading {file.name}...\")\n",
        "    try:\n",
        "        df = pd.read_csv(file, low_memory=False)\n",
        "        print(f\"  Shape: {df.shape}, Columns: {len(df.columns)}\")\n",
        "        dataframes.append(df)\n",
        "    except Exception as e:\n",
        "        print(f\"  Error loading {file.name}: {e}\")\n",
        "\n",
        "print(f\"\\nTotal dataframes loaded: {len(dataframes)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combine all dataframes\n",
        "print(\"Combining all dataframes...\")\n",
        "df = pd.concat(dataframes, ignore_index=True)\n",
        "print(f\"Combined dataset shape: {df.shape}\")\n",
        "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Explore Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for Label column and unique values\n",
        "if 'Label' in df.columns:\n",
        "    print(\"Label column found!\")\n",
        "    print(\"\\nUnique labels:\")\n",
        "    print(df['Label'].value_counts())\n",
        "    print(f\"\\nTotal unique labels: {df['Label'].nunique()}\")\n",
        "else:\n",
        "    print(\"Label column not found. Available columns:\")\n",
        "    print(df.columns.tolist())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "print(\"Missing values per column:\")\n",
        "missing = df.isnull().sum()\n",
        "missing_percent = (missing / len(df)) * 100\n",
        "missing_df = pd.DataFrame({\n",
        "    'Missing Count': missing,\n",
        "    'Missing Percent': missing_percent\n",
        "})\n",
        "missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
        "print(missing_df)\n",
        "print(f\"\\nTotal columns with missing values: {len(missing_df)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for infinite values\n",
        "print(\"Checking for infinite values...\")\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "inf_counts = {}\n",
        "for col in numeric_cols:\n",
        "    inf_count = np.isinf(df[col]).sum()\n",
        "    if inf_count > 0:\n",
        "        inf_counts[col] = inf_count\n",
        "\n",
        "if inf_counts:\n",
        "    print(\"Columns with infinite values:\")\n",
        "    for col, count in inf_counts.items():\n",
        "        print(f\"  {col}: {count}\")\n",
        "else:\n",
        "    print(\"No infinite values found.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Preprocess Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a copy for preprocessing\n",
        "df_processed = df.copy()\n",
        "print(f\"Starting preprocessing with shape: {df_processed.shape}\")\n",
        "\n",
        "# Handle infinite values - replace with NaN first, then handle NaN\n",
        "print(\"Handling infinite values...\")\n",
        "numeric_cols = df_processed.select_dtypes(include=[np.number]).columns\n",
        "for col in numeric_cols:\n",
        "    df_processed[col] = df_processed[col].replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "# Handle missing values - drop columns with too many missing values (>50%)\n",
        "print(\"Handling missing values...\")\n",
        "missing_threshold = 0.5\n",
        "cols_to_drop = []\n",
        "for col in df_processed.columns:\n",
        "    if col != 'Label':\n",
        "        missing_pct = df_processed[col].isnull().sum() / len(df_processed)\n",
        "        if missing_pct > missing_threshold:\n",
        "            cols_to_drop.append(col)\n",
        "\n",
        "if cols_to_drop:\n",
        "    print(f\"Dropping {len(cols_to_drop)} columns with >50% missing values\")\n",
        "    df_processed = df_processed.drop(columns=cols_to_drop)\n",
        "\n",
        "# For remaining numeric columns with missing values, fill with median\n",
        "numeric_cols = df_processed.select_dtypes(include=[np.number]).columns\n",
        "for col in numeric_cols:\n",
        "    if df_processed[col].isnull().sum() > 0:\n",
        "        df_processed[col].fillna(df_processed[col].median(), inplace=True)\n",
        "\n",
        "# Drop rows where Label is missing\n",
        "if 'Label' in df_processed.columns:\n",
        "    initial_rows = len(df_processed)\n",
        "    df_processed = df_processed.dropna(subset=['Label'])\n",
        "    dropped_rows = initial_rows - len(df_processed)\n",
        "    if dropped_rows > 0:\n",
        "        print(f\"Dropped {dropped_rows} rows with missing labels\")\n",
        "\n",
        "print(f\"\\nAfter preprocessing shape: {df_processed.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert Label to binary classification (BENIGN = 0, ATTACK = 1)\n",
        "print(\"Converting labels to binary classification...\")\n",
        "print(\"\\nOriginal label distribution:\")\n",
        "print(df_processed['Label'].value_counts())\n",
        "\n",
        "# Create binary label\n",
        "df_processed['Label_Binary'] = df_processed['Label'].apply(\n",
        "    lambda x: 0 if str(x).strip().upper() == 'BENIGN' else 1\n",
        ")\n",
        "\n",
        "print(\"\\nBinary label distribution:\")\n",
        "print(df_processed['Label_Binary'].value_counts())\n",
        "print(f\"\\nBENIGN (0): {(df_processed['Label_Binary'] == 0).sum()}\")\n",
        "print(f\"ATTACK (1): {(df_processed['Label_Binary'] == 1).sum()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Separate features and target\n",
        "X = df_processed.drop(columns=['Label', 'Label_Binary'])\n",
        "y = df_processed['Label_Binary']\n",
        "\n",
        "print(f\"Features shape: {X.shape}\")\n",
        "print(f\"Target shape: {y.shape}\")\n",
        "print(f\"\\nFeature columns: {len(X.columns)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Train-Test Split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, \n",
        "    test_size=0.2, \n",
        "    random_state=42, \n",
        "    stratify=y  # Maintain class distribution\n",
        ")\n",
        "\n",
        "print(f\"Training set shape: {X_train.shape}\")\n",
        "print(f\"Test set shape: {X_test.shape}\")\n",
        "print(f\"\\nTraining set label distribution:\")\n",
        "print(y_train.value_counts())\n",
        "print(f\"\\nTest set label distribution:\")\n",
        "print(y_test.value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Train Random Forest Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import time\n",
        "\n",
        "print(\"Training Random Forest Classifier...\")\n",
        "print(\"Initializing model with default parameters...\")\n",
        "\n",
        "# Initialize Random Forest with default parameters\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,  # Use all available CPUs\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "start_time = time.time()\n",
        "rf_model.fit(X_train, y_train)\n",
        "training_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\nTraining completed in {training_time:.2f} seconds\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Evaluate Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    classification_report, confusion_matrix, roc_auc_score\n",
        ")\n",
        "\n",
        "# Make predictions\n",
        "print(\"Making predictions on test set...\")\n",
        "y_pred = rf_model.predict(X_test)\n",
        "y_pred_proba = rf_model.predict_proba(X_test)[:, 1]  # Probability of class 1 (ATTACK)\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"MODEL PERFORMANCE METRICS\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "print(f\"F1-Score:  {f1:.4f}\")\n",
        "print(f\"ROC-AUC:   {roc_auc:.4f}\")\n",
        "print(\"=\"*50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Classification report\n",
        "print(\"\\nDetailed Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['BENIGN', 'ATTACK']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion Matrix\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "print(\"\\nInterpretation:\")\n",
        "print(f\"True Negatives (BENIGN correctly identified):  {cm[0][0]}\")\n",
        "print(f\"False Positives (BENIGN misclassified as ATTACK): {cm[0][1]}\")\n",
        "print(f\"False Negatives (ATTACK misclassified as BENIGN): {cm[1][0]}\")\n",
        "print(f\"True Positives (ATTACK correctly identified):  {cm[1][1]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature importance\n",
        "print(\"\\nTop 20 Most Important Features:\")\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': X.columns,\n",
        "    'importance': rf_model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(feature_importance.head(20).to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Save Model and Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import joblib\n",
        "from datetime import datetime\n",
        "\n",
        "# Create directories if they don't exist\n",
        "MODELS_DIR.mkdir(exist_ok=True)\n",
        "RESULTS_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# Save model\n",
        "model_filename = MODELS_DIR / 'random_forest_binary_20241211.joblib'\n",
        "joblib.dump(rf_model, model_filename)\n",
        "print(f\"Model saved to: {model_filename}\")\n",
        "\n",
        "# Save feature names (important for later inference)\n",
        "feature_names_file = MODELS_DIR / 'feature_names_binary.joblib'\n",
        "joblib.dump(list(X.columns), feature_names_file)\n",
        "print(f\"Feature names saved to: {feature_names_file}\")\n",
        "\n",
        "# Save results to CSV\n",
        "results = {\n",
        "    'Model': ['Random Forest (Binary)'],\n",
        "    'Accuracy': [accuracy],\n",
        "    'Precision': [precision],\n",
        "    'Recall': [recall],\n",
        "    'F1-Score': [f1],\n",
        "    'ROC-AUC': [roc_auc],\n",
        "    'Training Time (seconds)': [training_time],\n",
        "    'Training Samples': [len(X_train)],\n",
        "    'Test Samples': [len(X_test)],\n",
        "    'Date': [datetime.now().strftime('%Y-%m-%d %H:%M:%S')]\n",
        "}\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "results_file = RESULTS_DIR / 'random_forest_binary_results.csv'\n",
        "results_df.to_csv(results_file, index=False)\n",
        "print(f\"\\nResults saved to: {results_file}\")\n",
        "\n",
        "# Save feature importance\n",
        "importance_file = RESULTS_DIR / 'feature_importance_binary.csv'\n",
        "feature_importance.to_csv(importance_file, index=False)\n",
        "print(f\"Feature importance saved to: {importance_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Quick Verification Test\n",
        "\n",
        "Let's test the model on a sample from one of the original files to verify it's working correctly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick test on a small sample from one file\n",
        "print(\"Testing model on a small sample from Monday file...\")\n",
        "test_file = DATASET_DIR / 'Monday-WorkingHours.pcap_ISCX.csv'\n",
        "test_sample = pd.read_csv(test_file, nrows=1000, low_memory=False)  # Just 1000 rows for quick test\n",
        "\n",
        "# Preprocess the sample (same steps)\n",
        "numeric_cols_sample = test_sample.select_dtypes(include=[np.number]).columns\n",
        "for col in numeric_cols_sample:\n",
        "    if col in X.columns:\n",
        "        test_sample[col] = test_sample[col].replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "# Prepare features\n",
        "test_X_sample = test_sample[X.columns].copy()\n",
        "test_X_sample = test_X_sample.fillna(X_train.median())\n",
        "\n",
        "# Make predictions\n",
        "sample_predictions = rf_model.predict(test_X_sample)\n",
        "\n",
        "# Get actual labels if available\n",
        "if 'Label' in test_sample.columns:\n",
        "    test_y_sample = test_sample['Label'].apply(lambda x: 0 if str(x).strip().upper() == 'BENIGN' else 1)\n",
        "    sample_accuracy = accuracy_score(test_y_sample, sample_predictions)\n",
        "    print(f\"\\nSample Test Results:\")\n",
        "    print(f\"  Accuracy: {sample_accuracy:.4f} ({sample_accuracy*100:.2f}%)\")\n",
        "    print(f\"  Predictions - BENIGN: {(sample_predictions == 0).sum()}, ATTACK: {(sample_predictions == 1).sum()}\")\n",
        "    print(f\"  Actual - BENIGN: {(test_y_sample == 0).sum()}, ATTACK: {(test_y_sample == 1).sum()}\")\n",
        "else:\n",
        "    print(f\"\\nPredictions on sample:\")\n",
        "    print(f\"  BENIGN: {(sample_predictions == 0).sum()}, ATTACK: {(sample_predictions == 1).sum()}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"✅ Model training and verification complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "✅ **Model trained successfully!**\n",
        "\n",
        "**What we did:**\n",
        "1. Loaded and combined all CICIDS2017 CSV files\n",
        "2. Preprocessed data (handled missing/infinite values)\n",
        "3. Converted multi-class labels to binary (BENIGN vs ATTACK)\n",
        "4. Trained a Random Forest classifier\n",
        "5. Evaluated performance with comprehensive metrics\n",
        "6. Saved the model and results\n",
        "\n",
        "**Next Steps:**\n",
        "1. Review the metrics above - is the model detecting attacks well?\n",
        "2. Check the confusion matrix - are false positives/negatives acceptable?\n",
        "3. If results look good, we can proceed to multiclass classification\n",
        "4. If not, we can tune hyperparameters or try different preprocessing\n",
        "5. Once satisfied, we can prepare for integration into your web app\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
