{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# XGBoost Model Training on NFStream Features\n",
        "\n",
        "**Goal:** Train an XGBoost model on NFStream-extracted features and compare with Random Forest.\n",
        "\n",
        "## Why XGBoost?\n",
        "- Often 5-10% better accuracy than Random Forest\n",
        "- Better handling of complex patterns\n",
        "- Faster inference\n",
        "- Better for imbalanced datasets\n",
        "\n",
        "## Current Results:\n",
        "- Random Forest: 83.39% DDoS detection on Friday PCAP (501K flows)\n",
        "- Test Accuracy: 70%\n",
        "\n",
        "## Expected Improvement:\n",
        "- Target: 85-90% DDoS detection\n",
        "- Test Accuracy: 75-80%+\n",
        "\n",
        "## Approach:\n",
        "1. Load same NFStream-extracted features from previous training\n",
        "2. Train XGBoost model with hyperparameter tuning\n",
        "3. Compare with Random Forest\n",
        "4. Test on Friday PCAP\n",
        "5. Save best model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "import time\n",
        "from datetime import datetime\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set paths\n",
        "BASE_DIR = Path.cwd().parent\n",
        "DATA_DIR = BASE_DIR / 'data_processed'\n",
        "MODELS_DIR = BASE_DIR / 'models'\n",
        "RESULTS_DIR = BASE_DIR / 'results'\n",
        "PCAP_DIR = BASE_DIR / 'pcap'\n",
        "\n",
        "print(f\"Base directory: {BASE_DIR}\")\n",
        "print(f\"Data directory: {DATA_DIR}\")\n",
        "\n",
        "# Check if we have saved NFStream features from previous training\n",
        "monday_features = DATA_DIR / 'nfstream_monday_features.csv'\n",
        "friday_features = DATA_DIR / 'nfstream_friday_features.csv'\n",
        "\n",
        "if monday_features.exists() and friday_features.exists():\n",
        "    print(f\"\\n‚úÖ Found saved NFStream features from previous training\")\n",
        "    print(f\"   Monday: {monday_features}\")\n",
        "    print(f\"   Friday: {friday_features}\")\n",
        "    USE_SAVED = True\n",
        "else:\n",
        "    print(f\"\\n‚ö†Ô∏è  Saved features not found. Will extract from PCAP files...\")\n",
        "    USE_SAVED = False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load or Extract NFStream Features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if USE_SAVED:\n",
        "    # Load saved features (much faster!)\n",
        "    print(\"Loading saved NFStream features...\")\n",
        "    df_monday = pd.read_csv(monday_features)\n",
        "    df_friday = pd.read_csv(friday_features)\n",
        "    \n",
        "    print(f\"\\nMonday features: {df_monday.shape}\")\n",
        "    print(f\"Friday features: {df_friday.shape}\")\n",
        "    print(f\"\\nLabels:\")\n",
        "    print(f\"  Monday: {df_monday['Label'].value_counts().to_dict()}\")\n",
        "    print(f\"  Friday: {df_friday['Label'].value_counts().to_dict()}\")\n",
        "else:\n",
        "    # Extract from PCAP files (same as notebook 05)\n",
        "    print(\"Extracting features from PCAP files...\")\n",
        "    print(\"(This will take 30-60 minutes)\")\n",
        "    \n",
        "    try:\n",
        "        import nfstream\n",
        "        from nfstream import NFStreamer\n",
        "        \n",
        "        NFSTREAM_ATTRIBUTES = [\n",
        "            'dst_port',\n",
        "            'bidirectional_duration_ms',\n",
        "            'src2dst_packets', 'dst2src_packets', 'bidirectional_packets',\n",
        "            'src2dst_bytes', 'dst2src_bytes', 'bidirectional_bytes',\n",
        "            'src2dst_max_ps', 'src2dst_min_ps', 'src2dst_mean_ps', 'src2dst_stddev_ps',\n",
        "            'dst2src_max_ps', 'dst2src_min_ps', 'dst2src_mean_ps', 'dst2src_stddev_ps',\n",
        "            'bidirectional_min_ps', 'bidirectional_max_ps', 'bidirectional_mean_ps', 'bidirectional_stddev_ps',\n",
        "            'bidirectional_mean_piat_ms', 'bidirectional_stddev_piat_ms', 'bidirectional_max_piat_ms', 'bidirectional_min_piat_ms',\n",
        "            'src2dst_duration_ms', 'src2dst_mean_piat_ms', 'src2dst_stddev_piat_ms', 'src2dst_max_piat_ms', 'src2dst_min_piat_ms',\n",
        "            'dst2src_duration_ms', 'dst2src_mean_piat_ms', 'dst2src_stddev_piat_ms', 'dst2src_max_piat_ms', 'dst2src_min_piat_ms',\n",
        "            'src2dst_psh_packets', 'src2dst_urg_packets', 'src2dst_syn_packets', 'src2dst_fin_packets', 'src2dst_rst_packets', 'src2dst_ack_packets',\n",
        "            'dst2src_psh_packets', 'dst2src_urg_packets', 'dst2src_syn_packets', 'dst2src_fin_packets', 'dst2src_rst_packets', 'dst2src_ack_packets',\n",
        "        ]\n",
        "        \n",
        "        def extract_nfstream_features(pcap_path, max_flows=250000, label=None):\n",
        "            streamer = NFStreamer(\n",
        "                source=str(pcap_path),\n",
        "                statistical_analysis=True,\n",
        "                splt_analysis=0,\n",
        "                n_dissections=0,\n",
        "            )\n",
        "            \n",
        "            flows_list = []\n",
        "            for i, flow in enumerate(streamer):\n",
        "                flow_dict = {}\n",
        "                for attr in NFSTREAM_ATTRIBUTES:\n",
        "                    try:\n",
        "                        value = getattr(flow, attr, 0)\n",
        "                        flow_dict[attr] = 0 if value is None else value\n",
        "                    except:\n",
        "                        flow_dict[attr] = 0\n",
        "                \n",
        "                if label:\n",
        "                    flow_dict['Label'] = label\n",
        "                \n",
        "                flows_list.append(flow_dict)\n",
        "                \n",
        "                if (i + 1) % 25000 == 0:\n",
        "                    print(f\"  Processed {i+1:,} flows...\")\n",
        "                \n",
        "                if i + 1 >= max_flows:\n",
        "                    break\n",
        "            \n",
        "            return pd.DataFrame(flows_list)\n",
        "        \n",
        "        monday_pcap = PCAP_DIR / 'Monday-WorkingHours.pcap'\n",
        "        friday_pcap = PCAP_DIR / 'Friday-WorkingHours.pcap'\n",
        "        \n",
        "        print(\"\\nExtracting from Monday PCAP...\")\n",
        "        df_monday = extract_nfstream_features(monday_pcap, max_flows=250000, label='BENIGN')\n",
        "        \n",
        "        print(\"\\nExtracting from Friday PCAP...\")\n",
        "        df_friday = extract_nfstream_features(friday_pcap, max_flows=250000, label='DDoS')\n",
        "        \n",
        "        # Save for future use\n",
        "        df_monday.to_csv(monday_features, index=False)\n",
        "        df_friday.to_csv(friday_features, index=False)\n",
        "        print(\"\\n‚úÖ Features saved for future use\")\n",
        "        \n",
        "    except ImportError:\n",
        "        raise ImportError(\"NFStream not installed. Run: pip install nfstream\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Prepare Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combine datasets\n",
        "print(\"Combining datasets...\")\n",
        "df_combined = pd.concat([df_monday, df_friday], ignore_index=True)\n",
        "\n",
        "print(f\"\\nCombined shape: {df_combined.shape}\")\n",
        "print(f\"Label distribution:\")\n",
        "print(df_combined['Label'].value_counts())\n",
        "\n",
        "# Balance classes\n",
        "benign_count = (df_combined['Label'] == 'BENIGN').sum()\n",
        "ddos_count = (df_combined['Label'] == 'DDoS').sum()\n",
        "min_count = min(benign_count, ddos_count)\n",
        "\n",
        "print(f\"\\nBalancing to {min_count:,} samples each...\")\n",
        "df_benign = df_combined[df_combined['Label'] == 'BENIGN'].sample(n=min_count, random_state=42)\n",
        "df_ddos = df_combined[df_combined['Label'] == 'DDoS'].sample(n=min_count, random_state=42)\n",
        "df_balanced = pd.concat([df_benign, df_ddos], ignore_index=True).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(f\"Balanced shape: {df_balanced.shape}\")\n",
        "print(f\"Label distribution:\")\n",
        "print(df_balanced['Label'].value_counts())\n",
        "\n",
        "# Separate features and labels\n",
        "X = df_balanced.drop(columns=['Label'])\n",
        "y = df_balanced['Label']\n",
        "\n",
        "# Preprocess\n",
        "print(\"\\nPreprocessing...\")\n",
        "X = X.replace([np.inf, -np.inf], np.nan)\n",
        "X = X.fillna(X.median())\n",
        "X = X.fillna(0)\n",
        "\n",
        "print(f\"Final features shape: {X.shape}\")\n",
        "print(f\"Missing values: {X.isnull().sum().sum()}\")\n",
        "print(f\"Infinite values: {np.isinf(X.select_dtypes(include=[np.number])).sum().sum()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Train-Test Split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set: {X_train.shape}\")\n",
        "print(f\"Test set: {X_test.shape}\")\n",
        "print(f\"\\nTraining labels:\")\n",
        "print(y_train.value_counts())\n",
        "print(f\"\\nTest labels:\")\n",
        "print(y_test.value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Train XGBoost Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if XGBoost is installed\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    print(f\"‚úÖ XGBoost version: {xgb.__version__}\")\n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è XGBoost not installed. Installing...\")\n",
        "    import subprocess\n",
        "    subprocess.check_call(['pip', 'install', 'xgboost'])\n",
        "    import xgboost as xgb\n",
        "    print(f\"‚úÖ XGBoost installed: {xgb.__version__}\")\n",
        "\n",
        "# Convert labels to numeric for XGBoost\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "y_train_encoded = le.fit_transform(y_train)\n",
        "y_test_encoded = le.transform(y_test)\n",
        "\n",
        "# Class mapping\n",
        "class_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
        "print(f\"\\nClass mapping: {class_mapping}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Training XGBoost Model\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# XGBoost parameters (optimized for binary classification)\n",
        "xgb_model = xgb.XGBClassifier(\n",
        "    n_estimators=200,  # More trees\n",
        "    max_depth=10,\n",
        "    learning_rate=0.1,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    min_child_weight=3,\n",
        "    gamma=0.1,\n",
        "    reg_alpha=0.1,\n",
        "    reg_lambda=1.0,\n",
        "    scale_pos_weight=1.0,  # Balanced classes\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    eval_metric='logloss',\n",
        "    verbosity=1\n",
        ")\n",
        "\n",
        "# Train\n",
        "start_time = time.time()\n",
        "xgb_model.fit(\n",
        "    X_train, y_train_encoded,\n",
        "    eval_set=[(X_test, y_test_encoded)],\n",
        "    early_stopping_rounds=20,\n",
        "    verbose=True\n",
        ")\n",
        "training_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\n‚úÖ Training completed in {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    classification_report, confusion_matrix\n",
        ")\n",
        "\n",
        "# Predictions\n",
        "y_pred_xgb = xgb_model.predict(X_test)\n",
        "y_pred_proba_xgb = xgb_model.predict_proba(X_test)\n",
        "\n",
        "# Convert back to original labels\n",
        "y_pred_labels = le.inverse_transform(y_pred_xgb)\n",
        "\n",
        "# Metrics\n",
        "accuracy_xgb = accuracy_score(y_test, y_pred_labels)\n",
        "precision_xgb = precision_score(y_test, y_pred_labels, pos_label='DDoS', zero_division=0)\n",
        "recall_xgb = recall_score(y_test, y_pred_labels, pos_label='DDoS', zero_division=0)\n",
        "f1_xgb = f1_score(y_test, y_pred_labels, pos_label='DDoS', zero_division=0)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"XGBOOST MODEL PERFORMANCE (Test Set)\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Accuracy:  {accuracy_xgb:.4f} ({accuracy_xgb*100:.2f}%)\")\n",
        "print(f\"Precision: {precision_xgb:.4f}\")\n",
        "print(f\"Recall:    {recall_xgb:.4f}\")\n",
        "print(f\"F1-Score:  {f1_xgb:.4f}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_labels))\n",
        "\n",
        "# Confusion matrix\n",
        "cm_xgb = confusion_matrix(y_test, y_pred_labels)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm_xgb)\n",
        "print(f\"\\nTrue Negatives (BENIGN correctly identified):  {cm_xgb[0][0]}\")\n",
        "print(f\"False Positives (BENIGN misclassified as DDoS): {cm_xgb[0][1]}\")\n",
        "print(f\"False Negatives (DDoS misclassified as BENIGN): {cm_xgb[1][0]}\")\n",
        "print(f\"True Positives (DDoS correctly identified):  {cm_xgb[1][1]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Random Forest model for comparison\n",
        "import joblib\n",
        "\n",
        "rf_model_path = MODELS_DIR / 'random_forest_nfstream_from_scratch.joblib'\n",
        "if rf_model_path.exists():\n",
        "    print(\"Loading Random Forest model for comparison...\")\n",
        "    rf_model = joblib.load(rf_model_path)\n",
        "    \n",
        "    # Make predictions\n",
        "    y_pred_rf = rf_model.predict(X_test)\n",
        "    \n",
        "    # Metrics\n",
        "    accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "    precision_rf = precision_score(y_test, y_pred_rf, pos_label='DDoS', zero_division=0)\n",
        "    recall_rf = recall_score(y_test, y_pred_rf, pos_label='DDoS', zero_division=0)\n",
        "    f1_rf = f1_score(y_test, y_pred_rf, pos_label='DDoS', zero_division=0)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"MODEL COMPARISON\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"{'Metric':<15} {'Random Forest':<20} {'XGBoost':<20} {'Improvement':<15}\")\n",
        "    print(\"-\"*60)\n",
        "    print(f\"{'Accuracy':<15} {accuracy_rf:.4f} ({accuracy_rf*100:.2f}%){'':<6} {accuracy_xgb:.4f} ({accuracy_xgb*100:.2f}%){'':<6} {(accuracy_xgb-accuracy_rf)*100:+.2f}%\")\n",
        "    print(f\"{'Precision':<15} {precision_rf:.4f}{'':<16} {precision_xgb:.4f}{'':<16} {(precision_xgb-precision_rf)*100:+.2f}%\")\n",
        "    print(f\"{'Recall':<15} {recall_rf:.4f}{'':<16} {recall_xgb:.4f}{'':<16} {(recall_xgb-recall_rf)*100:+.2f}%\")\n",
        "    print(f\"{'F1-Score':<15} {f1_rf:.4f}{'':<16} {f1_xgb:.4f}{'':<16} {(f1_xgb-f1_rf)*100:+.2f}%\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    if accuracy_xgb > accuracy_rf:\n",
        "        improvement = (accuracy_xgb - accuracy_rf) / accuracy_rf * 100\n",
        "        print(f\"\\n‚úÖ XGBoost is {improvement:.2f}% better than Random Forest!\")\n",
        "    else:\n",
        "        print(f\"\\n‚ö†Ô∏è Random Forest performs slightly better on test set\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Random Forest model not found for comparison\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Test on Friday PCAP (Real-World Validation)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test on actual Friday PCAP file\n",
        "print(\"Testing XGBoost model on Friday PCAP file...\")\n",
        "print(\"(This will extract features and make predictions)\")\n",
        "\n",
        "from nfstream import NFStreamer\n",
        "\n",
        "friday_pcap = PCAP_DIR / 'Friday-WorkingHours.pcap'\n",
        "if not friday_pcap.exists():\n",
        "    print(f\"‚ö†Ô∏è Friday PCAP not found: {friday_pcap}\")\n",
        "else:\n",
        "    print(f\"\\nExtracting features from: {friday_pcap.name}\")\n",
        "    \n",
        "    # Extract test flows (different portion than training)\n",
        "    NFSTREAM_ATTRIBUTES = list(X.columns)  # Use same features as training\n",
        "    \n",
        "    test_streamer = NFStreamer(\n",
        "        source=str(friday_pcap),\n",
        "        statistical_analysis=True,\n",
        "        splt_analysis=0,\n",
        "        n_dissections=0,\n",
        "    )\n",
        "    \n",
        "    # Extract flows starting from 500K (skip training portion)\n",
        "    test_flows = []\n",
        "    skip_first = 500000\n",
        "    test_count = 50000\n",
        "    \n",
        "    print(f\"Skipping first {skip_first:,} flows, extracting {test_count:,} test flows...\")\n",
        "    \n",
        "    for i, flow in enumerate(test_streamer):\n",
        "        if i < skip_first:\n",
        "            continue\n",
        "        \n",
        "        flow_dict = {}\n",
        "        for attr in NFSTREAM_ATTRIBUTES:\n",
        "            try:\n",
        "                value = getattr(flow, attr, 0)\n",
        "                flow_dict[attr] = 0 if value is None else value\n",
        "            except:\n",
        "                flow_dict[attr] = 0\n",
        "        \n",
        "        test_flows.append(flow_dict)\n",
        "        \n",
        "        if (i - skip_first + 1) % 10000 == 0:\n",
        "            print(f\"  Extracted {i - skip_first + 1:,} flows...\")\n",
        "        \n",
        "        if len(test_flows) >= test_count:\n",
        "            break\n",
        "    \n",
        "    df_test_pcap = pd.DataFrame(test_flows)\n",
        "    print(f\"\\n‚úÖ Extracted {len(df_test_pcap):,} test flows\")\n",
        "    \n",
        "    # Preprocess\n",
        "    X_test_pcap = df_test_pcap.copy()\n",
        "    X_test_pcap = X_test_pcap.replace([np.inf, -np.inf], np.nan)\n",
        "    X_test_pcap = X_test_pcap.fillna(X_train.median())\n",
        "    X_test_pcap = X_test_pcap.fillna(0)\n",
        "    X_test_pcap = X_test_pcap[X_train.columns]  # Ensure same order\n",
        "    \n",
        "    # Predictions\n",
        "    print(\"\\nMaking predictions...\")\n",
        "    test_predictions_xgb = xgb_model.predict(X_test_pcap)\n",
        "    test_predictions_labels = le.inverse_transform(test_predictions_xgb)\n",
        "    \n",
        "    # Compare with RF if available\n",
        "    if rf_model_path.exists():\n",
        "        test_predictions_rf = rf_model.predict(X_test_pcap)\n",
        "    \n",
        "    # Results\n",
        "    pred_counts_xgb = pd.Series(test_predictions_labels).value_counts()\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"FRIDAY PCAP TEST RESULTS\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Total flows: {len(test_predictions_labels):,}\")\n",
        "    print(f\"\\nXGBoost Predictions:\")\n",
        "    for label, count in pred_counts_xgb.items():\n",
        "        pct = count / len(test_predictions_labels) * 100\n",
        "        print(f\"  {label}: {count:,} ({pct:.2f}%)\")\n",
        "    \n",
        "    if rf_model_path.exists():\n",
        "        pred_counts_rf = pd.Series(test_predictions_rf).value_counts()\n",
        "        print(f\"\\nRandom Forest Predictions:\")\n",
        "        for label, count in pred_counts_rf.items():\n",
        "            pct = count / len(test_predictions_rf) * 100\n",
        "            print(f\"  {label}: {count:,} ({pct:.2f}%)\")\n",
        "        \n",
        "        # Compare DDoS detection\n",
        "        ddos_xgb = pred_counts_xgb.get('DDoS', 0) / len(test_predictions_labels) * 100\n",
        "        ddos_rf = pred_counts_rf.get('DDoS', 0) / len(test_predictions_rf) * 100\n",
        "        \n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(\"DDoS Detection Comparison:\")\n",
        "        print(f\"  XGBoost:     {ddos_xgb:.2f}%\")\n",
        "        print(f\"  Random Forest: {ddos_rf:.2f}%\")\n",
        "        print(f\"  Improvement:   {ddos_xgb - ddos_rf:+.2f}%\")\n",
        "        \n",
        "        if ddos_xgb > ddos_rf:\n",
        "            print(f\"\\n‚úÖ XGBoost detects {ddos_xgb - ddos_rf:.2f}% more DDoS attacks!\")\n",
        "        elif ddos_xgb < ddos_rf:\n",
        "            print(f\"\\n‚ö†Ô∏è Random Forest detects {ddos_rf - ddos_xgb:.2f}% more DDoS attacks\")\n",
        "        else:\n",
        "            print(f\"\\n‚úì Both models perform similarly\")\n",
        "    \n",
        "    ddos_detected_xgb = pred_counts_xgb.get('DDoS', 0)\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    if ddos_detected_xgb > len(test_predictions_labels) * 0.5:\n",
        "        print(\"‚úÖ SUCCESS! XGBoost correctly detects DDoS attacks in Friday PCAP!\")\n",
        "        print(f\"   Detected {ddos_detected_xgb:,} DDoS flows ({ddos_detected_xgb/len(test_predictions_labels)*100:.1f}%)\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Low DDoS detection - may need more tuning\")\n",
        "    print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Save Best Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save XGBoost model if it performs better\n",
        "if 'accuracy_xgb' in dir() and 'accuracy_rf' in dir():\n",
        "    if accuracy_xgb > accuracy_rf:\n",
        "        print(\"Saving XGBoost model (performs better than Random Forest)...\")\n",
        "        \n",
        "        # Save model\n",
        "        model_filename = MODELS_DIR / 'xgboost_nfstream_from_scratch.joblib'\n",
        "        joblib.dump(xgb_model, model_filename)\n",
        "        print(f\"‚úÖ Model saved: {model_filename}\")\n",
        "        \n",
        "        # Save feature names\n",
        "        feature_names_file = MODELS_DIR / 'feature_names_nfstream_xgboost.joblib'\n",
        "        joblib.dump(list(X.columns), feature_names_file)\n",
        "        print(f\"‚úÖ Feature names saved: {feature_names_file}\")\n",
        "        \n",
        "        # Save class names and label encoder\n",
        "        class_names_file = MODELS_DIR / 'class_names_nfstream_xgboost.joblib'\n",
        "        joblib.dump(['BENIGN', 'DDoS'], class_names_file)\n",
        "        print(f\"‚úÖ Class names saved: {class_names_file}\")\n",
        "        \n",
        "        label_encoder_file = MODELS_DIR / 'label_encoder_nfstream_xgboost.joblib'\n",
        "        joblib.dump(le, label_encoder_file)\n",
        "        print(f\"‚úÖ Label encoder saved: {label_encoder_file}\")\n",
        "        \n",
        "        print(\"\\nüéâ XGBoost model saved successfully!\")\n",
        "        print(\"   Update predictor.py to use this model for better performance!\")\n",
        "    else:\n",
        "        print(\"Random Forest performs better - keeping Random Forest as primary model\")\n",
        "        print(\"XGBoost model available but not saved as primary\")\n",
        "else:\n",
        "    print(\"Saving XGBoost model...\")\n",
        "    \n",
        "    model_filename = MODELS_DIR / 'xgboost_nfstream_from_scratch.joblib'\n",
        "    joblib.dump(xgb_model, model_filename)\n",
        "    print(f\"‚úÖ Model saved: {model_filename}\")\n",
        "    \n",
        "    feature_names_file = MODELS_DIR / 'feature_names_nfstream_xgboost.joblib'\n",
        "    joblib.dump(list(X.columns), feature_names_file)\n",
        "    print(f\"‚úÖ Feature names saved: {feature_names_file}\")\n",
        "    \n",
        "    class_names_file = MODELS_DIR / 'class_names_nfstream_xgboost.joblib'\n",
        "    joblib.dump(['BENIGN', 'DDoS'], class_names_file)\n",
        "    print(f\"‚úÖ Class names saved: {class_names_file}\")\n",
        "    \n",
        "    label_encoder_file = MODELS_DIR / 'label_encoder_nfstream_xgboost.joblib'\n",
        "    joblib.dump(le, label_encoder_file)\n",
        "    print(f\"‚úÖ Label encoder saved: {label_encoder_file}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
