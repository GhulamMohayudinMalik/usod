{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get all CSV files\n",
        "csv_files = sorted(DATASET_DIR.glob('*.csv'))\n",
        "print(f\"Found {len(csv_files)} CSV files\")\n",
        "\n",
        "# Load all CSV files\n",
        "dataframes = []\n",
        "for file in csv_files:\n",
        "    print(f\"Loading {file.name}...\")\n",
        "    try:\n",
        "        df_temp = pd.read_csv(file, low_memory=False)\n",
        "        # Strip whitespace from column names\n",
        "        df_temp.columns = df_temp.columns.str.strip()\n",
        "        print(f\"  Shape: {df_temp.shape}, Columns: {len(df_temp.columns)}\")\n",
        "        dataframes.append(df_temp)\n",
        "    except Exception as e:\n",
        "        print(f\"  Error loading {file.name}: {e}\")\n",
        "\n",
        "print(f\"\\nTotal dataframes loaded: {len(dataframes)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combine all dataframes\n",
        "print(\"Combining all dataframes...\")\n",
        "df = pd.concat(dataframes, ignore_index=True)\n",
        "print(f\"Combined dataset shape: {df.shape}\")\n",
        "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Explore Labels and Handle Class Imbalance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find label column\n",
        "label_col = None\n",
        "for col in df.columns:\n",
        "    if col.strip().upper() == 'LABEL':\n",
        "        label_col = col\n",
        "        break\n",
        "\n",
        "if not label_col:\n",
        "    raise ValueError(\"Label column not found!\")\n",
        "\n",
        "print(\"Label distribution (all classes):\")\n",
        "print(\"=\"*70)\n",
        "label_counts = df[label_col].value_counts().sort_index()\n",
        "print(label_counts)\n",
        "print(f\"\\nTotal unique labels: {len(label_counts)}\")\n",
        "print(f\"\\nClass distribution percentages:\")\n",
        "print((label_counts / len(df) * 100).round(2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for very rare classes (less than 100 samples)\n",
        "print(\"\\nChecking for very rare classes (less than 100 samples)...\")\n",
        "rare_classes = label_counts[label_counts < 100]\n",
        "print(f\"\\nRare classes (< 100 samples): {len(rare_classes)}\")\n",
        "if len(rare_classes) > 0:\n",
        "    print(rare_classes)\n",
        "    print(f\"\\nTotal samples in rare classes: {rare_classes.sum()}\")\n",
        "    print(\"\\nStrategy: We'll combine rare classes into 'Other' category\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Preprocess Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a copy for preprocessing\n",
        "df_processed = df.copy()\n",
        "print(f\"Starting preprocessing with shape: {df_processed.shape}\")\n",
        "\n",
        "# Ensure column names are stripped\n",
        "df_processed.columns = df_processed.columns.str.strip()\n",
        "\n",
        "# Find label column\n",
        "label_col = None\n",
        "for col in df_processed.columns:\n",
        "    if col.strip().upper() == 'LABEL':\n",
        "        label_col = col\n",
        "        break\n",
        "\n",
        "# Handle infinite values - replace with NaN first, then handle NaN\n",
        "print(\"Handling infinite values...\")\n",
        "numeric_cols = df_processed.select_dtypes(include=[np.number]).columns\n",
        "for col in numeric_cols:\n",
        "    df_processed[col] = df_processed[col].replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "# Handle missing values - drop columns with too many missing values (>50%)\n",
        "print(\"Handling missing values...\")\n",
        "missing_threshold = 0.5\n",
        "cols_to_drop = []\n",
        "for col in df_processed.columns:\n",
        "    if col != label_col and label_col is not None:\n",
        "        missing_pct = df_processed[col].isnull().sum() / len(df_processed)\n",
        "        if missing_pct > missing_threshold:\n",
        "            cols_to_drop.append(col)\n",
        "\n",
        "if cols_to_drop:\n",
        "    print(f\"Dropping {len(cols_to_drop)} columns with >50% missing values\")\n",
        "    df_processed = df_processed.drop(columns=cols_to_drop)\n",
        "\n",
        "# For remaining numeric columns with missing values, fill with median\n",
        "numeric_cols = df_processed.select_dtypes(include=[np.number]).columns\n",
        "for col in numeric_cols:\n",
        "    if df_processed[col].isnull().sum() > 0:\n",
        "        df_processed[col].fillna(df_processed[col].median(), inplace=True)\n",
        "\n",
        "# Drop rows where Label is missing\n",
        "if label_col:\n",
        "    initial_rows = len(df_processed)\n",
        "    df_processed = df_processed.dropna(subset=[label_col])\n",
        "    dropped_rows = initial_rows - len(df_processed)\n",
        "    if dropped_rows > 0:\n",
        "        print(f\"Dropped {dropped_rows} rows with missing labels\")\n",
        "\n",
        "print(f\"\\nAfter preprocessing shape: {df_processed.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Handle class imbalance: combine very rare classes (less than 100 samples) into \"Other\"\n",
        "print(\"Handling class imbalance...\")\n",
        "label_counts_processed = df_processed[label_col].value_counts()\n",
        "rare_threshold = 100\n",
        "rare_classes_list = label_counts_processed[label_counts_processed < rare_threshold].index.tolist()\n",
        "\n",
        "if len(rare_classes_list) > 0:\n",
        "    print(f\"\\nCombining {len(rare_classes_list)} rare classes into 'Other':\")\n",
        "    for cls in rare_classes_list:\n",
        "        print(f\"  - {cls}: {label_counts_processed[cls]} samples\")\n",
        "    \n",
        "    # Replace rare classes with \"Other\"\n",
        "    df_processed['Label_Multiclass'] = df_processed[label_col].copy()\n",
        "    df_processed.loc[df_processed[label_col].isin(rare_classes_list), 'Label_Multiclass'] = 'Other'\n",
        "    \n",
        "    print(f\"\\nAfter combining rare classes:\")\n",
        "    print(df_processed['Label_Multiclass'].value_counts().sort_index())\n",
        "else:\n",
        "    print(\"No rare classes found, using original labels\")\n",
        "    df_processed['Label_Multiclass'] = df_processed[label_col].copy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Separate features and target\n",
        "cols_to_drop = ['Label_Multiclass']\n",
        "if label_col:\n",
        "    cols_to_drop.append(label_col)\n",
        "\n",
        "X = df_processed.drop(columns=cols_to_drop)\n",
        "y = df_processed['Label_Multiclass']\n",
        "\n",
        "print(f\"Features shape: {X.shape}\")\n",
        "print(f\"Target shape: {y.shape}\")\n",
        "print(f\"\\nNumber of classes: {y.nunique()}\")\n",
        "print(f\"\\nClass distribution:\")\n",
        "print(y.value_counts().sort_index())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
